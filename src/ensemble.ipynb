{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET THIS VARIABLE IF \n",
    "explained_model_backend = 'tensorflow' # 'sklearn' or 'tensorflow'\n",
    "\n",
    "# WARNING REMEMEBER TO CHANGE MANUALLY CFEC MODEL LOADING IF SOME CHANGES APPEAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hours.per.week', 'age', 'capital.loss', 'education.num',\n",
       "       'capital.gain', 'workclass', 'marital.status', 'occupation', 'race',\n",
       "       'sex', 'native.country', 'income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from utils.transformations import min_max_normalization, inverse_min_max_normalization, transform_to_sparse, inverse_transform_to_sparse\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Change cwd\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "os.getcwd()\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning) #Ignore sklearn \"RF fitted with FeatureNames\"\n",
    "\n",
    "train_dataset = pd.read_csv(\"data/adult.csv\")\n",
    "dataset_name = 'adult'\n",
    "instance_to_explain_index = 8908\n",
    "\n",
    "with open('data/adult_constraints.json', 'r') as f:\n",
    "    constr = json.load(f)\n",
    "\n",
    "if explained_model_backend == 'sklearn':\n",
    "    # SKLEARN\n",
    "    with open('models/adult_RF.pkl', 'rb') as f:\n",
    "        explained_model = pickle.load(f)\n",
    "else: \n",
    "    # TENSORFLOW\n",
    "    explained_model = tf.keras.models.load_model('models/adult_NN/')\n",
    "\n",
    "\n",
    "train_dataset = train_dataset[constr['features_order_nonsplit']]\n",
    "train_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionable_mask_indices_sparse = [1 if any([act in x for act in constr['actionable_features']]) else 0 for x in constr['features_order_after_split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_instance = train_dataset.drop(columns=\"income\")[instance_to_explain_index:instance_to_explain_index+1]\n",
    "\n",
    "all_counterfactuals = pd.DataFrame(columns=train_dataset.columns.tolist() + ['explainer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>age</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>education.num</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>workclass</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>native.country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8908</th>\n",
       "      <td>28</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      hours.per.week  age  capital.loss  education.num  capital.gain  \\\n",
       "8908              28   32             0              9             0   \n",
       "\n",
       "     workclass marital.status     occupation   race     sex native.country  \n",
       "8908   Private       Divorced  Other-service  Other  Female  United-States  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dataset to sparse\n",
    "train_dataset_sparse = transform_to_sparse(\n",
    "    _df=train_dataset.drop(columns=\"income\"),\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    categorical_features=constr['categorical_features_nonsplit'],\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "# Min-max normalization\n",
    "train_dataset_sparse_normalized = min_max_normalization(\n",
    "    _df=train_dataset_sparse,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "query_instance_sparse_normalized = train_dataset_sparse_normalized[instance_to_explain_index:instance_to_explain_index+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 102ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.18133777, 0.81866217]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_model.predict(query_instance_sparse_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.65s/it]\n"
     ]
    }
   ],
   "source": [
    "from dice import DiceModel\n",
    "\n",
    "if explained_model_backend == 'sklearn':\n",
    "    dice_model = DiceModel(\n",
    "        train_dataset=train_dataset,\n",
    "        continuous_features=constr['continuous_features_nonsplit'],\n",
    "        categorical_features=constr['categorical_features_nonsplit'],\n",
    "        target=constr['target_feature'],\n",
    "        backend='sklearn',\n",
    "        model=explained_model\n",
    "    )\n",
    "else:\n",
    "    dice_model = DiceModel(\n",
    "        train_dataset=train_dataset,\n",
    "        continuous_features=constr['continuous_features_nonsplit'],\n",
    "        categorical_features=constr['categorical_features_nonsplit'],\n",
    "        target=constr['target_feature'],\n",
    "        backend='TF2',\n",
    "        model=explained_model\n",
    "    )\n",
    "\n",
    "dice_counterfactuals_df = dice_model.generate_counterfactuals(\n",
    "    query_instance=query_instance,\n",
    "    total_CFs=20,\n",
    "    desired_class='opposite',\n",
    "    features_to_vary=constr['actionable_features'],\n",
    "    permitted_range=constr['feature_ranges'],\n",
    ")\n",
    "\n",
    "dice_counterfactuals_df['explainer'] = 'dice'\n",
    "all_counterfactuals = pd.concat([all_counterfactuals, dice_counterfactuals_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>age</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>education.num</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>workclass</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "      <th>explainer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>51761.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>dice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51501.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>dice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Married-spouse-absent</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>dice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Other</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>dice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Without-pay</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>dice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>dice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Never-worked</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>dice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>?</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Priv-house-serv</td>\n",
       "      <td>Other</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "      <td>dice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hours.per.week   age capital.loss  education.num  capital.gain  \\\n",
       "0            28.0  32.0            0            3.0       51761.0   \n",
       "1            28.0  32.0            0            2.0       51501.0   \n",
       "2            28.0  32.0            0            9.0           0.0   \n",
       "3             3.0  32.0            0            9.0           0.0   \n",
       "4            28.0  32.0            0            9.0           0.0   \n",
       "5            28.0  30.0            0            9.0           0.0   \n",
       "6            28.0  32.0            0            7.0           0.0   \n",
       "7            28.0  32.0            0            9.0           0.0   \n",
       "\n",
       "      workclass         marital.status        occupation   race     sex  \\\n",
       "0       Private               Divorced     Other-service  Other  Female   \n",
       "1       Private               Divorced     Other-service  Other  Female   \n",
       "2     State-gov  Married-spouse-absent     Other-service  Other  Female   \n",
       "3       Private               Divorced  Transport-moving  Other  Female   \n",
       "4   Without-pay              Separated     Other-service  Other  Female   \n",
       "5       Private               Divorced     Other-service  Other  Female   \n",
       "6  Never-worked               Divorced     Other-service  Other  Female   \n",
       "7             ?               Divorced   Priv-house-serv  Other  Female   \n",
       "\n",
       "  native.country income explainer  \n",
       "0  United-States      0      dice  \n",
       "1  United-States      0      dice  \n",
       "2  United-States      0      dice  \n",
       "3  United-States      0      dice  \n",
       "4  United-States      0      dice  \n",
       "5  United-States      0      dice  \n",
       "6  United-States      0      dice  \n",
       "7  United-States      0      dice  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_counterfactuals.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(actionable_mask_indices_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to change ['hours.per.week', 'age', 'capital.loss', 'education.num', 'capital.gain', 'workclass_?', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc', 'workclass_Self-emp-not-inc', 'workclass_State-gov', 'workclass_Without-pay', 'marital.status_Divorced', 'marital.status_Married-AF-spouse', 'marital.status_Married-civ-spouse', 'marital.status_Married-spouse-absent', 'marital.status_Never-married', 'marital.status_Separated', 'marital.status_Widowed', 'occupation_?', 'occupation_Adm-clerical', 'occupation_Armed-Forces', 'occupation_Craft-repair', 'occupation_Exec-managerial', 'occupation_Farming-fishing', 'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct', 'occupation_Other-service', 'occupation_Priv-house-serv', 'occupation_Prof-specialty', 'occupation_Protective-serv', 'occupation_Sales', 'occupation_Tech-support', 'occupation_Transport-moving']\n"
     ]
    }
   ],
   "source": [
    "columns_to_change_str = train_dataset_sparse_normalized.columns.to_numpy()[np.array(actionable_mask_indices_sparse, dtype='bool')].tolist()\n",
    "print('Columns to change', columns_to_change_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n"
     ]
    }
   ],
   "source": [
    "print(np.where(actionable_mask_indices_sparse)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraints: \n",
      " [Freeze(columns=['race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black', 'race_Other', 'race_White']), Freeze(columns=['sex_Female', 'sex_Male']), Freeze(columns=['native.country_?', 'native.country_Cambodia', 'native.country_Canada', 'native.country_China', 'native.country_Columbia', 'native.country_Cuba', 'native.country_Dominican-Republic', 'native.country_Ecuador', 'native.country_El-Salvador', 'native.country_England', 'native.country_France', 'native.country_Germany', 'native.country_Greece', 'native.country_Guatemala', 'native.country_Haiti', 'native.country_Holand-Netherlands', 'native.country_Honduras', 'native.country_Hong', 'native.country_Hungary', 'native.country_India', 'native.country_Iran', 'native.country_Ireland', 'native.country_Italy', 'native.country_Jamaica', 'native.country_Japan', 'native.country_Laos', 'native.country_Mexico', 'native.country_Nicaragua', 'native.country_Outlying-US(Guam-USVI-etc)', 'native.country_Peru', 'native.country_Philippines', 'native.country_Poland', 'native.country_Portugal', 'native.country_Puerto-Rico', 'native.country_Scotland', 'native.country_South', 'native.country_Taiwan', 'native.country_Thailand', 'native.country_Trinadad&Tobago', 'native.country_United-States', 'native.country_Vietnam', 'native.country_Yugoslavia']), OneHot(name='workclass', start_column=6, end_column=14), OneHot(name='marital.status', start_column=15, end_column=21), OneHot(name='occupation', start_column=22, end_column=36), OneHot(name='race', start_column=37, end_column=41), OneHot(name='sex', start_column=42, end_column=43), OneHot(name='native.country', start_column=44, end_column=85), ValueRange(columns=['age'], min_value=17, max_value=99), ValueRange(columns=['education.num'], min_value=1, max_value=16), ValueRange(columns=['capital.gain'], min_value=0, max_value=99999), ValueRange(columns=['capital.loss'], min_value=0, max_value=4356), ValueRange(columns=['hours.per.week'], min_value=1, max_value=99)]\n",
      "1018/1018 [==============================] - 1s 1ms/step\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Continous columns: ['hours.per.week', 'age', 'capital.loss', 'education.num', 'capital.gain', 'workclass_?', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc', 'workclass_Self-emp-not-inc', 'workclass_State-gov', 'workclass_Without-pay', 'marital.status_Divorced', 'marital.status_Married-AF-spouse', 'marital.status_Married-civ-spouse', 'marital.status_Married-spouse-absent', 'marital.status_Never-married', 'marital.status_Separated', 'marital.status_Widowed', 'occupation_?', 'occupation_Adm-clerical', 'occupation_Armed-Forces', 'occupation_Craft-repair', 'occupation_Exec-managerial', 'occupation_Farming-fishing', 'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct', 'occupation_Other-service', 'occupation_Priv-house-serv', 'occupation_Prof-specialty', 'occupation_Protective-serv', 'occupation_Sales', 'occupation_Tech-support', 'occupation_Transport-moving', 'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black', 'race_Other', 'race_White', 'sex_Female', 'sex_Male', 'native.country_?', 'native.country_Cambodia', 'native.country_Canada', 'native.country_China', 'native.country_Columbia', 'native.country_Cuba', 'native.country_Dominican-Republic', 'native.country_Ecuador', 'native.country_El-Salvador', 'native.country_England', 'native.country_France', 'native.country_Germany', 'native.country_Greece', 'native.country_Guatemala', 'native.country_Haiti', 'native.country_Holand-Netherlands', 'native.country_Honduras', 'native.country_Hong', 'native.country_Hungary', 'native.country_India', 'native.country_Iran', 'native.country_Ireland', 'native.country_Italy', 'native.country_Jamaica', 'native.country_Japan', 'native.country_Laos', 'native.country_Mexico', 'native.country_Nicaragua', 'native.country_Outlying-US(Guam-USVI-etc)', 'native.country_Peru', 'native.country_Philippines', 'native.country_Poland', 'native.country_Portugal', 'native.country_Puerto-Rico', 'native.country_Scotland', 'native.country_South', 'native.country_Taiwan', 'native.country_Thailand', 'native.country_Trinadad&Tobago', 'native.country_United-States', 'native.country_Vietnam', 'native.country_Yugoslavia']\n",
      "[]\n",
      "<keras.engine.functional.Functional object at 0x000001B41AD684C0>\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Continous columns: ['hours.per.week', 'age', 'capital.loss', 'education.num', 'capital.gain', 'workclass_?', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc', 'workclass_Self-emp-not-inc', 'workclass_State-gov', 'workclass_Without-pay', 'marital.status_Divorced', 'marital.status_Married-AF-spouse', 'marital.status_Married-civ-spouse', 'marital.status_Married-spouse-absent', 'marital.status_Never-married', 'marital.status_Separated', 'marital.status_Widowed', 'occupation_?', 'occupation_Adm-clerical', 'occupation_Armed-Forces', 'occupation_Craft-repair', 'occupation_Exec-managerial', 'occupation_Farming-fishing', 'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct', 'occupation_Other-service', 'occupation_Priv-house-serv', 'occupation_Prof-specialty', 'occupation_Protective-serv', 'occupation_Sales', 'occupation_Tech-support', 'occupation_Transport-moving', 'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black', 'race_Other', 'race_White', 'sex_Female', 'sex_Male', 'native.country_?', 'native.country_Cambodia', 'native.country_Canada', 'native.country_China', 'native.country_Columbia', 'native.country_Cuba', 'native.country_Dominican-Republic', 'native.country_Ecuador', 'native.country_El-Salvador', 'native.country_England', 'native.country_France', 'native.country_Germany', 'native.country_Greece', 'native.country_Guatemala', 'native.country_Haiti', 'native.country_Holand-Netherlands', 'native.country_Honduras', 'native.country_Hong', 'native.country_Hungary', 'native.country_India', 'native.country_Iran', 'native.country_Ireland', 'native.country_Italy', 'native.country_Jamaica', 'native.country_Japan', 'native.country_Laos', 'native.country_Mexico', 'native.country_Nicaragua', 'native.country_Outlying-US(Guam-USVI-etc)', 'native.country_Peru', 'native.country_Philippines', 'native.country_Poland', 'native.country_Portugal', 'native.country_Puerto-Rico', 'native.country_Scotland', 'native.country_South', 'native.country_Taiwan', 'native.country_Thailand', 'native.country_Trinadad&Tobago', 'native.country_United-States', 'native.country_Vietnam', 'native.country_Yugoslavia']\n",
      "[]\n",
      "<keras.engine.functional.Functional object at 0x000001B418077970>\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Continous columns: ['hours.per.week', 'age', 'capital.loss', 'education.num', 'capital.gain', 'workclass_?', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc', 'workclass_Self-emp-not-inc', 'workclass_State-gov', 'workclass_Without-pay', 'marital.status_Divorced', 'marital.status_Married-AF-spouse', 'marital.status_Married-civ-spouse', 'marital.status_Married-spouse-absent', 'marital.status_Never-married', 'marital.status_Separated', 'marital.status_Widowed', 'occupation_?', 'occupation_Adm-clerical', 'occupation_Armed-Forces', 'occupation_Craft-repair', 'occupation_Exec-managerial', 'occupation_Farming-fishing', 'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct', 'occupation_Other-service', 'occupation_Priv-house-serv', 'occupation_Prof-specialty', 'occupation_Protective-serv', 'occupation_Sales', 'occupation_Tech-support', 'occupation_Transport-moving', 'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black', 'race_Other', 'race_White', 'sex_Female', 'sex_Male', 'native.country_?', 'native.country_Cambodia', 'native.country_Canada', 'native.country_China', 'native.country_Columbia', 'native.country_Cuba', 'native.country_Dominican-Republic', 'native.country_Ecuador', 'native.country_El-Salvador', 'native.country_England', 'native.country_France', 'native.country_Germany', 'native.country_Greece', 'native.country_Guatemala', 'native.country_Haiti', 'native.country_Holand-Netherlands', 'native.country_Honduras', 'native.country_Hong', 'native.country_Hungary', 'native.country_India', 'native.country_Iran', 'native.country_Ireland', 'native.country_Italy', 'native.country_Jamaica', 'native.country_Japan', 'native.country_Laos', 'native.country_Mexico', 'native.country_Nicaragua', 'native.country_Outlying-US(Guam-USVI-etc)', 'native.country_Peru', 'native.country_Philippines', 'native.country_Poland', 'native.country_Portugal', 'native.country_Puerto-Rico', 'native.country_Scotland', 'native.country_South', 'native.country_Taiwan', 'native.country_Thailand', 'native.country_Trinadad&Tobago', 'native.country_United-States', 'native.country_Vietnam', 'native.country_Yugoslavia']\n",
      "[]\n",
      "<keras.engine.functional.Functional object at 0x000001B41AA11DB0>\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Continous columns: ['hours.per.week', 'age', 'capital.loss', 'education.num', 'capital.gain', 'workclass_?', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc', 'workclass_Self-emp-not-inc', 'workclass_State-gov', 'workclass_Without-pay', 'marital.status_Divorced', 'marital.status_Married-AF-spouse', 'marital.status_Married-civ-spouse', 'marital.status_Married-spouse-absent', 'marital.status_Never-married', 'marital.status_Separated', 'marital.status_Widowed', 'occupation_?', 'occupation_Adm-clerical', 'occupation_Armed-Forces', 'occupation_Craft-repair', 'occupation_Exec-managerial', 'occupation_Farming-fishing', 'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct', 'occupation_Other-service', 'occupation_Priv-house-serv', 'occupation_Prof-specialty', 'occupation_Protective-serv', 'occupation_Sales', 'occupation_Tech-support', 'occupation_Transport-moving', 'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black', 'race_Other', 'race_White', 'sex_Female', 'sex_Male', 'native.country_?', 'native.country_Cambodia', 'native.country_Canada', 'native.country_China', 'native.country_Columbia', 'native.country_Cuba', 'native.country_Dominican-Republic', 'native.country_Ecuador', 'native.country_El-Salvador', 'native.country_England', 'native.country_France', 'native.country_Germany', 'native.country_Greece', 'native.country_Guatemala', 'native.country_Haiti', 'native.country_Holand-Netherlands', 'native.country_Honduras', 'native.country_Hong', 'native.country_Hungary', 'native.country_India', 'native.country_Iran', 'native.country_Ireland', 'native.country_Italy', 'native.country_Jamaica', 'native.country_Japan', 'native.country_Laos', 'native.country_Mexico', 'native.country_Nicaragua', 'native.country_Outlying-US(Guam-USVI-etc)', 'native.country_Peru', 'native.country_Philippines', 'native.country_Poland', 'native.country_Portugal', 'native.country_Puerto-Rico', 'native.country_Scotland', 'native.country_South', 'native.country_Taiwan', 'native.country_Thailand', 'native.country_Trinadad&Tobago', 'native.country_United-States', 'native.country_Vietnam', 'native.country_Yugoslavia']\n",
      "[]\n",
      "<keras.engine.functional.Functional object at 0x000001B41AC1A860>\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Continous columns: ['hours.per.week', 'age', 'capital.loss', 'education.num', 'capital.gain', 'workclass_?', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc', 'workclass_Self-emp-not-inc', 'workclass_State-gov', 'workclass_Without-pay', 'marital.status_Divorced', 'marital.status_Married-AF-spouse', 'marital.status_Married-civ-spouse', 'marital.status_Married-spouse-absent', 'marital.status_Never-married', 'marital.status_Separated', 'marital.status_Widowed', 'occupation_?', 'occupation_Adm-clerical', 'occupation_Armed-Forces', 'occupation_Craft-repair', 'occupation_Exec-managerial', 'occupation_Farming-fishing', 'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct', 'occupation_Other-service', 'occupation_Priv-house-serv', 'occupation_Prof-specialty', 'occupation_Protective-serv', 'occupation_Sales', 'occupation_Tech-support', 'occupation_Transport-moving', 'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black', 'race_Other', 'race_White', 'sex_Female', 'sex_Male', 'native.country_?', 'native.country_Cambodia', 'native.country_Canada', 'native.country_China', 'native.country_Columbia', 'native.country_Cuba', 'native.country_Dominican-Republic', 'native.country_Ecuador', 'native.country_El-Salvador', 'native.country_England', 'native.country_France', 'native.country_Germany', 'native.country_Greece', 'native.country_Guatemala', 'native.country_Haiti', 'native.country_Holand-Netherlands', 'native.country_Honduras', 'native.country_Hong', 'native.country_Hungary', 'native.country_India', 'native.country_Iran', 'native.country_Ireland', 'native.country_Italy', 'native.country_Jamaica', 'native.country_Japan', 'native.country_Laos', 'native.country_Mexico', 'native.country_Nicaragua', 'native.country_Outlying-US(Guam-USVI-etc)', 'native.country_Peru', 'native.country_Philippines', 'native.country_Poland', 'native.country_Portugal', 'native.country_Puerto-Rico', 'native.country_Scotland', 'native.country_South', 'native.country_Taiwan', 'native.country_Thailand', 'native.country_Trinadad&Tobago', 'native.country_United-States', 'native.country_Vietnam', 'native.country_Yugoslavia']\n",
      "[]\n",
      "<keras.engine.functional.Functional object at 0x000001B418124940>\n"
     ]
    }
   ],
   "source": [
    "from cfec_ece import CfecEceModel \n",
    "\n",
    "train_dataset_sparse_normalized_subsample = train_dataset_sparse_normalized.sample(frac=1.0)\n",
    "\n",
    "\n",
    "cfec_model = CfecEceModel(\n",
    "    train_data_normalized=train_dataset_sparse_normalized,\n",
    "    constraints_dictionary=constr,\n",
    "    model = explained_model,\n",
    "    columns_to_change=actionable_mask_indices_sparse,\n",
    "    cadex_max_feature_changes=15,\n",
    "    cadex_max_epochs=20\n",
    ")\n",
    "\n",
    "cfec_model.fit(\n",
    "    fimap_load_string=f'{dataset_name}_{explained_model_backend}|2023-01-17'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.functional.Functional object at 0x000001B41AD684C0>\n",
      "BCE <modules.CFEC.cfec.explainers._fimap.Fimap object at 0x000001B419797160> generated 1 counterfactuals\n",
      "<keras.engine.functional.Functional object at 0x000001B418077970>\n",
      "BCE <modules.CFEC.cfec.explainers._fimap.Fimap object at 0x000001B41AD68A90> generated 1 counterfactuals\n",
      "<keras.engine.functional.Functional object at 0x000001B41AA11DB0>\n",
      "BCE <modules.CFEC.cfec.explainers._fimap.Fimap object at 0x000001B4180F0550> generated 1 counterfactuals\n",
      "<keras.engine.functional.Functional object at 0x000001B41AC1A860>\n",
      "BCE <modules.CFEC.cfec.explainers._fimap.Fimap object at 0x000001B412B8B9A0> generated 1 counterfactuals\n",
      "<keras.engine.functional.Functional object at 0x000001B418124940>\n",
      "BCE <modules.CFEC.cfec.explainers._fimap.Fimap object at 0x000001B41AEB78B0> generated 1 counterfactuals\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "BCE <modules.CFEC.cfec.explainers._cadex.Cadex object at 0x000001B41ADAE830> generated 1 counterfactuals\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "BCE <modules.CFEC.cfec.explainers._cadex.Cadex object at 0x000001B41ADAD3C0> generated 1 counterfactuals\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "BCE <modules.CFEC.cfec.explainers._cadex.Cadex object at 0x000001B41ADAFFA0> generated 1 counterfactuals\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "BCE <modules.CFEC.cfec.explainers._cadex.Cadex object at 0x000001B41AB4F0D0> generated 1 counterfactuals\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "BCE <modules.CFEC.cfec.explainers._cadex.Cadex object at 0x000001B41AD82500> generated 1 counterfactuals\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "BCE <modules.CFEC.cfec.explainers._cadex.Cadex object at 0x000001B41AD820E0> generated 1 counterfactuals\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "BCE <modules.CFEC.cfec.explainers._cadex.Cadex object at 0x000001B41AD80880> generated 1 counterfactuals\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "BCE <modules.CFEC.cfec.explainers._cadex.Cadex object at 0x000001B41AD83970> generated 1 counterfactuals\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "BCE <modules.CFEC.cfec.explainers._cadex.Cadex object at 0x000001B41AD83CA0> generated 1 counterfactuals\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "BCE <modules.CFEC.cfec.explainers._cadex.Cadex object at 0x000001B41AD80280> generated 1 counterfactuals\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "BCE <modules.CFEC.cfec.explainers._cadex.Cadex object at 0x000001B41AD819C0> generated 1 counterfactuals\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "BCE <modules.CFEC.cfec.explainers._cadex.Cadex object at 0x000001B41AD82E00> generated 1 counterfactuals\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "BCE <modules.CFEC.cfec.explainers._cadex.Cadex object at 0x000001B41AD83130> generated 1 counterfactuals\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "BCE <modules.CFEC.cfec.explainers._cadex.Cadex object at 0x000001B41AD83E20> generated 1 counterfactuals\n",
      "[[ 1.59545624e-01  8.91411131e-02  0.00000000e+00 ...  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.89434757e-01  1.19150357e-01  0.00000000e+00 ...  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.91824383e-01  1.21783339e-01  0.00000000e+00 ...  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 2.84312695e-01  1.05784864e-01 -2.21034538e-04 ...  1.00000000e+00\n",
      "  -4.70555010e-11 -1.12357044e-11]\n",
      " [ 2.84312695e-01  1.05784864e-01 -2.21034538e-04 ...  1.00000000e+00\n",
      "  -4.70555010e-11 -1.12357044e-11]\n",
      " [ 2.84312695e-01  1.05784864e-01 -2.21034538e-04 ...  1.00000000e+00\n",
      "  -4.70555010e-11 -1.12357044e-11]]\n",
      "[[ 1.59545624e-01  8.91411131e-02  0.00000000e+00 ...  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.89434757e-01  1.19150357e-01  0.00000000e+00 ...  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 1.91824383e-01  1.21783339e-01  0.00000000e+00 ...  1.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 2.84312695e-01  1.05784864e-01 -2.21034538e-04 ...  1.00000000e+00\n",
      "  -4.70555010e-11 -1.12357044e-11]\n",
      " [ 2.84312695e-01  1.05784864e-01 -2.21034538e-04 ...  1.00000000e+00\n",
      "  -4.70555010e-11 -1.12357044e-11]\n",
      " [ 2.84312695e-01  1.05784864e-01 -2.21034538e-04 ...  1.00000000e+00\n",
      "  -4.70555010e-11 -1.12357044e-11]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>age</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>education.num</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>workclass_?</th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Never-worked</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>...</th>\n",
       "      <th>native.country_Portugal</th>\n",
       "      <th>native.country_Puerto-Rico</th>\n",
       "      <th>native.country_Scotland</th>\n",
       "      <th>native.country_South</th>\n",
       "      <th>native.country_Taiwan</th>\n",
       "      <th>native.country_Thailand</th>\n",
       "      <th>native.country_Trinadad&amp;Tobago</th>\n",
       "      <th>native.country_United-States</th>\n",
       "      <th>native.country_Vietnam</th>\n",
       "      <th>native.country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.159546</td>\n",
       "      <td>0.089141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.189435</td>\n",
       "      <td>0.119150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>-0.087225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.191824</td>\n",
       "      <td>0.121783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>-0.083728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.191862</td>\n",
       "      <td>0.121823</td>\n",
       "      <td>-0.083658</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>-0.083681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.191916</td>\n",
       "      <td>0.121880</td>\n",
       "      <td>-0.083601</td>\n",
       "      <td>0.449630</td>\n",
       "      <td>-0.083615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.225510</td>\n",
       "      <td>0.155479</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.225510</td>\n",
       "      <td>0.155479</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.225510</td>\n",
       "      <td>0.155479</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.275510</td>\n",
       "      <td>0.087264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.284313</td>\n",
       "      <td>0.105785</td>\n",
       "      <td>-0.000221</td>\n",
       "      <td>0.537048</td>\n",
       "      <td>-0.001780</td>\n",
       "      <td>-0.040689</td>\n",
       "      <td>0.029483</td>\n",
       "      <td>0.064279</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.697030</td>\n",
       "      <td>...</td>\n",
       "      <td>2.851869e-12</td>\n",
       "      <td>-5.311182e-11</td>\n",
       "      <td>-9.715559e-12</td>\n",
       "      <td>-1.571294e-11</td>\n",
       "      <td>9.338692e-12</td>\n",
       "      <td>-1.077733e-11</td>\n",
       "      <td>6.132937e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.705550e-11</td>\n",
       "      <td>-1.123570e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.284313</td>\n",
       "      <td>0.105785</td>\n",
       "      <td>-0.000221</td>\n",
       "      <td>0.537048</td>\n",
       "      <td>-0.001780</td>\n",
       "      <td>-0.040689</td>\n",
       "      <td>0.029483</td>\n",
       "      <td>0.064279</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.697030</td>\n",
       "      <td>...</td>\n",
       "      <td>2.851869e-12</td>\n",
       "      <td>-5.311182e-11</td>\n",
       "      <td>-9.715559e-12</td>\n",
       "      <td>-1.571294e-11</td>\n",
       "      <td>9.338692e-12</td>\n",
       "      <td>-1.077733e-11</td>\n",
       "      <td>6.132937e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.705550e-11</td>\n",
       "      <td>-1.123570e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.284313</td>\n",
       "      <td>0.105785</td>\n",
       "      <td>-0.000221</td>\n",
       "      <td>0.537048</td>\n",
       "      <td>-0.001780</td>\n",
       "      <td>-0.040689</td>\n",
       "      <td>0.029483</td>\n",
       "      <td>0.064279</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>1.156572</td>\n",
       "      <td>...</td>\n",
       "      <td>2.851869e-12</td>\n",
       "      <td>-5.311182e-11</td>\n",
       "      <td>-9.715559e-12</td>\n",
       "      <td>-1.571294e-11</td>\n",
       "      <td>9.338692e-12</td>\n",
       "      <td>-1.077733e-11</td>\n",
       "      <td>6.132937e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.705550e-11</td>\n",
       "      <td>-1.123570e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.284313</td>\n",
       "      <td>0.105785</td>\n",
       "      <td>-0.000221</td>\n",
       "      <td>0.537048</td>\n",
       "      <td>-0.001780</td>\n",
       "      <td>-0.040689</td>\n",
       "      <td>0.029483</td>\n",
       "      <td>0.064279</td>\n",
       "      <td>0.014876</td>\n",
       "      <td>0.697030</td>\n",
       "      <td>...</td>\n",
       "      <td>2.851869e-12</td>\n",
       "      <td>-5.311182e-11</td>\n",
       "      <td>-9.715559e-12</td>\n",
       "      <td>-1.571294e-11</td>\n",
       "      <td>9.338692e-12</td>\n",
       "      <td>-1.077733e-11</td>\n",
       "      <td>6.132937e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.705550e-11</td>\n",
       "      <td>-1.123570e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.284313</td>\n",
       "      <td>0.105785</td>\n",
       "      <td>-0.000221</td>\n",
       "      <td>0.537048</td>\n",
       "      <td>-0.001780</td>\n",
       "      <td>-0.040689</td>\n",
       "      <td>0.029483</td>\n",
       "      <td>0.064279</td>\n",
       "      <td>0.014876</td>\n",
       "      <td>0.697030</td>\n",
       "      <td>...</td>\n",
       "      <td>2.851869e-12</td>\n",
       "      <td>-5.311182e-11</td>\n",
       "      <td>-9.715559e-12</td>\n",
       "      <td>-1.571294e-11</td>\n",
       "      <td>9.338692e-12</td>\n",
       "      <td>-1.077733e-11</td>\n",
       "      <td>6.132937e-12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.705550e-11</td>\n",
       "      <td>-1.123570e-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    hours.per.week       age  capital.loss  education.num  capital.gain  \\\n",
       "0         0.159546  0.089141      0.000000       0.533333      0.000000   \n",
       "1         0.189435  0.119150      0.000000       0.533333     -0.087225   \n",
       "2         0.191824  0.121783      0.000000       0.533333     -0.083728   \n",
       "3         0.191862  0.121823     -0.083658       0.533333     -0.083681   \n",
       "4         0.191916  0.121880     -0.083601       0.449630     -0.083615   \n",
       "5         0.225510  0.155479     -0.050000       0.483333     -0.050000   \n",
       "6         0.225510  0.155479     -0.050000       0.483333     -0.050000   \n",
       "7         0.225510  0.155479     -0.050000       0.483333     -0.050000   \n",
       "8         0.275510  0.087264      0.000000       0.533333      0.000000   \n",
       "9         0.284313  0.105785     -0.000221       0.537048     -0.001780   \n",
       "10        0.284313  0.105785     -0.000221       0.537048     -0.001780   \n",
       "11        0.284313  0.105785     -0.000221       0.537048     -0.001780   \n",
       "12        0.284313  0.105785     -0.000221       0.537048     -0.001780   \n",
       "13        0.284313  0.105785     -0.000221       0.537048     -0.001780   \n",
       "\n",
       "    workclass_?  workclass_Federal-gov  workclass_Local-gov  \\\n",
       "0      0.000000               0.000000             0.000000   \n",
       "1      0.000000               0.000000             0.000000   \n",
       "2      0.000000               0.000000             0.000000   \n",
       "3      0.000000               0.000000             0.000000   \n",
       "4      0.000000               0.000000             0.000000   \n",
       "5      0.000000               0.000000             0.000000   \n",
       "6      0.000000               0.000000             0.000000   \n",
       "7      0.050000               0.000000             0.000000   \n",
       "8      0.000000               0.000000             0.000000   \n",
       "9     -0.040689               0.029483             0.064279   \n",
       "10    -0.040689               0.029483             0.064279   \n",
       "11    -0.040689               0.029483             0.064279   \n",
       "12    -0.040689               0.029483             0.064279   \n",
       "13    -0.040689               0.029483             0.064279   \n",
       "\n",
       "    workclass_Never-worked  workclass_Private  ...  native.country_Portugal  \\\n",
       "0                 0.000000           1.000000  ...             0.000000e+00   \n",
       "1                 0.000000           1.000000  ...             0.000000e+00   \n",
       "2                 0.000000           1.000000  ...             0.000000e+00   \n",
       "3                 0.000000           1.000000  ...             0.000000e+00   \n",
       "4                 0.000000           1.000000  ...             0.000000e+00   \n",
       "5                 0.000000           0.000000  ...             0.000000e+00   \n",
       "6                 0.000000           1.000000  ...             0.000000e+00   \n",
       "7                 0.000000           0.000000  ...             0.000000e+00   \n",
       "8                 0.000000           1.000000  ...             0.000000e+00   \n",
       "9                 0.000215           0.697030  ...             2.851869e-12   \n",
       "10                0.000215           0.697030  ...             2.851869e-12   \n",
       "11                0.000215           1.156572  ...             2.851869e-12   \n",
       "12                0.014876           0.697030  ...             2.851869e-12   \n",
       "13                0.014876           0.697030  ...             2.851869e-12   \n",
       "\n",
       "    native.country_Puerto-Rico  native.country_Scotland  native.country_South  \\\n",
       "0                 0.000000e+00             0.000000e+00          0.000000e+00   \n",
       "1                 0.000000e+00             0.000000e+00          0.000000e+00   \n",
       "2                 0.000000e+00             0.000000e+00          0.000000e+00   \n",
       "3                 0.000000e+00             0.000000e+00          0.000000e+00   \n",
       "4                 0.000000e+00             0.000000e+00          0.000000e+00   \n",
       "5                 0.000000e+00             0.000000e+00          0.000000e+00   \n",
       "6                 0.000000e+00             0.000000e+00          0.000000e+00   \n",
       "7                 0.000000e+00             0.000000e+00          0.000000e+00   \n",
       "8                 0.000000e+00             0.000000e+00          0.000000e+00   \n",
       "9                -5.311182e-11            -9.715559e-12         -1.571294e-11   \n",
       "10               -5.311182e-11            -9.715559e-12         -1.571294e-11   \n",
       "11               -5.311182e-11            -9.715559e-12         -1.571294e-11   \n",
       "12               -5.311182e-11            -9.715559e-12         -1.571294e-11   \n",
       "13               -5.311182e-11            -9.715559e-12         -1.571294e-11   \n",
       "\n",
       "    native.country_Taiwan  native.country_Thailand  \\\n",
       "0            0.000000e+00             0.000000e+00   \n",
       "1            0.000000e+00             0.000000e+00   \n",
       "2            0.000000e+00             0.000000e+00   \n",
       "3            0.000000e+00             0.000000e+00   \n",
       "4            0.000000e+00             0.000000e+00   \n",
       "5            0.000000e+00             0.000000e+00   \n",
       "6            0.000000e+00             0.000000e+00   \n",
       "7            0.000000e+00             0.000000e+00   \n",
       "8            0.000000e+00             0.000000e+00   \n",
       "9            9.338692e-12            -1.077733e-11   \n",
       "10           9.338692e-12            -1.077733e-11   \n",
       "11           9.338692e-12            -1.077733e-11   \n",
       "12           9.338692e-12            -1.077733e-11   \n",
       "13           9.338692e-12            -1.077733e-11   \n",
       "\n",
       "    native.country_Trinadad&Tobago  native.country_United-States  \\\n",
       "0                     0.000000e+00                           1.0   \n",
       "1                     0.000000e+00                           1.0   \n",
       "2                     0.000000e+00                           1.0   \n",
       "3                     0.000000e+00                           1.0   \n",
       "4                     0.000000e+00                           1.0   \n",
       "5                     0.000000e+00                           1.0   \n",
       "6                     0.000000e+00                           1.0   \n",
       "7                     0.000000e+00                           1.0   \n",
       "8                     0.000000e+00                           1.0   \n",
       "9                     6.132937e-12                           1.0   \n",
       "10                    6.132937e-12                           1.0   \n",
       "11                    6.132937e-12                           1.0   \n",
       "12                    6.132937e-12                           1.0   \n",
       "13                    6.132937e-12                           1.0   \n",
       "\n",
       "    native.country_Vietnam  native.country_Yugoslavia  \n",
       "0             0.000000e+00               0.000000e+00  \n",
       "1             0.000000e+00               0.000000e+00  \n",
       "2             0.000000e+00               0.000000e+00  \n",
       "3             0.000000e+00               0.000000e+00  \n",
       "4             0.000000e+00               0.000000e+00  \n",
       "5             0.000000e+00               0.000000e+00  \n",
       "6             0.000000e+00               0.000000e+00  \n",
       "7             0.000000e+00               0.000000e+00  \n",
       "8             0.000000e+00               0.000000e+00  \n",
       "9            -4.705550e-11              -1.123570e-11  \n",
       "10           -4.705550e-11              -1.123570e-11  \n",
       "11           -4.705550e-11              -1.123570e-11  \n",
       "12           -4.705550e-11              -1.123570e-11  \n",
       "13           -4.705550e-11              -1.123570e-11  \n",
       "\n",
       "[14 rows x 85 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfec_counterfactuals_raw, list_cfs_explainers = cfec_model.generate_counterfactuals(query_instance=query_instance_sparse_normalized.iloc[0])\n",
    "cfec_counterfactuals_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Research\\ecemosp\\src\\utils\\transformations.py:67: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dense_df[feature] = sparse_df[split_columns].idxmax(axis=1).str.replace(feature + '_', '')\n",
      "d:\\Research\\ecemosp\\src\\utils\\transformations.py:67: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dense_df[feature] = sparse_df[split_columns].idxmax(axis=1).str.replace(feature + '_', '')\n"
     ]
    }
   ],
   "source": [
    "# Do not allow for negative values\n",
    "cfec_counterfactuals_raw[cfec_counterfactuals_raw < 0] = 0\n",
    "\n",
    "# Inverse min-max normalization\n",
    "cfec_counterfactuals = inverse_min_max_normalization(\n",
    "    _df=cfec_counterfactuals_raw,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "# Inverse transform to sparse\n",
    "cfec_counterfactuals = inverse_transform_to_sparse(\n",
    "    sparse_df=cfec_counterfactuals,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    categorical_features=constr['categorical_features_nonsplit'],\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>age</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>education.num</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>workclass</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>native.country</th>\n",
       "      <th>explainer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Married-AF-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>cadex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Married-AF-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>cadex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Married-AF-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>cadex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Married-AF-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>cadex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Married-AF-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>cadex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>cadex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>cadex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>cadex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Married-AF-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>cadex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>fimap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>fimap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>fimap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>fimap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Private</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Other</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>fimap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hours.per.week  age  capital.loss  education.num  capital.gain workclass  \\\n",
       "0               16   23             0              9             0   Private   \n",
       "1               19   25             0              9             0   Private   \n",
       "2               19   25             0              9             0   Private   \n",
       "3               19   25             0              9             0   Private   \n",
       "4               19   25             0              7             0   Private   \n",
       "5               23   28             0              8             0         ?   \n",
       "6               23   28             0              8             0   Private   \n",
       "7               23   28             0              8             0         ?   \n",
       "8               27   23             0              9             0   Private   \n",
       "9               28   24             0              9             0   Private   \n",
       "10              28   24             0              9             0   Private   \n",
       "11              28   24             0              9             0   Private   \n",
       "12              28   24             0              9             0   Private   \n",
       "13              28   24             0              9             0   Private   \n",
       "\n",
       "        marital.status     occupation   race     sex native.country explainer  \n",
       "0    Married-AF-spouse  Other-service  Other    Male  United-States     cadex  \n",
       "1    Married-AF-spouse  Other-service  Other    Male  United-States     cadex  \n",
       "2    Married-AF-spouse  Other-service  Other    Male  United-States     cadex  \n",
       "3    Married-AF-spouse  Other-service  Other    Male  United-States     cadex  \n",
       "4    Married-AF-spouse  Other-service  Other    Male  United-States     cadex  \n",
       "5             Divorced  Other-service  Other    Male  United-States     cadex  \n",
       "6        Never-married  Other-service  Other    Male  United-States     cadex  \n",
       "7             Divorced  Other-service  Other    Male  United-States     cadex  \n",
       "8    Married-AF-spouse  Other-service  Other    Male  United-States     cadex  \n",
       "9        Never-married  Other-service  Other  Female  United-States     fimap  \n",
       "10  Married-civ-spouse  Other-service  Other  Female  United-States     fimap  \n",
       "11       Never-married  Other-service  Other  Female  United-States     fimap  \n",
       "12  Married-civ-spouse  Other-service  Other  Female  United-States     fimap  \n",
       "13  Married-civ-spouse  Other-service  Other  Female  United-States     fimap  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_cfs_explainers = list(map(lambda x: 'cadex' if 'cadex' in str.lower(x) else 'fimap', list_cfs_explainers))\n",
    "cfec_counterfactuals['explainer'] = list_cfs_explainers\n",
    "all_counterfactuals = pd.concat([all_counterfactuals, cfec_counterfactuals], ignore_index=True)\n",
    "cfec_counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_instance_sparse_normalized[cfec_counterfactuals_raw.columns[39:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfec_counterfactuals_raw[cfec_counterfactuals_raw.columns[39:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WACHTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ranges = (\n",
    "    train_dataset_sparse_normalized.to_numpy().min(axis=0),\n",
    "    train_dataset_sparse_normalized.to_numpy().max(axis=0),\n",
    ")\n",
    "non_actionable_indices = ~np.array(actionable_mask_indices_sparse, dtype='bool')\n",
    "feature_ranges[0][non_actionable_indices] = query_instance_sparse_normalized.to_numpy()[0][non_actionable_indices]\n",
    "feature_ranges[1][non_actionable_indices] = query_instance_sparse_normalized.to_numpy()[0][non_actionable_indices]\n",
    "feature_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_impl import AlibiWachter\n",
    "\n",
    "continous = len(constr['continuous_features_nonsplit'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if explained_model_backend == 'sklearn':\n",
    "    max_iter = 100\n",
    "    max_lam_steps=10\n",
    "    lam_init=0.001\n",
    "    lr_init=0.1\n",
    "    early_stop=50\n",
    "    tolerance=0.4\n",
    "    target_proba=1.0\n",
    "\n",
    "    wachter_model = AlibiWachter('../models/adult_RF.pkl', 'sklearn', \n",
    "    query_instance_sparse_normalized.shape, feature_ranges=feature_ranges,\n",
    "    max_iter=max_iter, max_lam_steps=max_lam_steps, lam_init=lam_init, \n",
    "    learning_rate_init=lr_init, early_stop=early_stop, tolerance=tolerance,\n",
    "    target_proba=target_proba,\n",
    "    )\n",
    "else:\n",
    "    #eps_wachter = np.array([[0.01] * continous + [0.01] * (len(train_dataset_sparse_normalized.columns) - continous)]) * (np.array(actionable_mask_indices_sparse, dtype=int) + 0.001)\n",
    "    wachter_model = AlibiWachter('../models/adult_NN/', 'tensorflow', query_instance_sparse_normalized.shape, target_proba=1.0, feature_ranges=feature_ranges)\n",
    "    \n",
    "explanation = wachter_model.generate_counterfactuals(query_instance_sparse_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.cf['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.cf['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wachter_counterfactuals = [explanation.cf['X']]\n",
    "for key, lst in explanation['data']['all'].items():\n",
    "    if lst:\n",
    "        for cf in lst:\n",
    "            wachter_counterfactuals.append(cf['X'])\n",
    "\n",
    "wachter_counterfactuals = np.array(wachter_counterfactuals).reshape(-1, query_instance_sparse_normalized.shape[1])\n",
    "\n",
    "wachter_counterfactuals_df = pd.DataFrame(wachter_counterfactuals, columns=constr['features_order_after_split'])\n",
    "\n",
    "# Inverse min-max normalization\n",
    "wachter_counterfactuals_df = inverse_min_max_normalization(\n",
    "    _df=wachter_counterfactuals_df,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "# Inverse transform to sparse\n",
    "wachter_counterfactuals_df = inverse_transform_to_sparse(\n",
    "    sparse_df=wachter_counterfactuals_df,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    categorical_features=constr['categorical_features_nonsplit'],\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "wachter_counterfactuals_df['explainer'] = 'wachter'\n",
    "\n",
    "# Reduce number of Wachter counterfactuals because they are almost the same\n",
    "sampled_wachter_cfs = wachter_counterfactuals_df.sample(min(len(wachter_counterfactuals_df), 10))\n",
    "\n",
    "#sampled_wachter_cfs = sampled_wachter_cfs.append(wachter_counterfactuals_df.iloc[0])\n",
    "\n",
    "all_counterfactuals = pd.concat([all_counterfactuals, wachter_counterfactuals_df.iloc[0:1], sampled_wachter_cfs], ignore_index=True)\n",
    "\n",
    "wachter_counterfactuals_df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wachter_counterfactuals_df.iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wachter_counterfactuals_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi.explainers import CEM\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "if explained_model_backend == 'sklearn':\n",
    "    # SKLEARN\n",
    "    with open('../models/adult_RF.pkl', 'rb') as f:\n",
    "        explained_model = pickle.load(f)\n",
    "else: \n",
    "    # TENSORFLOW\n",
    "    explained_model = tf.keras.models.load_model('../models/adult_NN/')\n",
    "\n",
    "shape = query_instance_sparse_normalized.shape  # instance shape\n",
    "continous = len(constr['continuous_features_nonsplit'])\n",
    "clip = (-1000.,1000.)\n",
    "eps_cem = (\n",
    "        0.05,\n",
    "        np.array([[0.05] * continous + [1.0] * (len(train_dataset_sparse_normalized.columns) - continous)]) #* (np.array(actionable_mask_indices_sparse) + 0.001) # Dont allow changes on non-actionable features\n",
    "        )\n",
    "# eps_cem = (0.1, 0.1)\n",
    "\n",
    "if explained_model_backend == 'sklearn':\n",
    "    mode = 'PN'\n",
    "    #feature_range = (train_dataset_sparse_normalized.to_numpy().min(),  # feature range for the perturbed instance\n",
    "    #                   train_dataset_sparse_normalized.to_numpy().max()) \n",
    "    update_num_grad = 2\n",
    "    c_init = 15.  # initial weight c of the loss term encouraging to predict a different class (PN) or\n",
    "                # the same class (PP) for the perturbed instance compared to the original instance to be explained\n",
    "    # Return probabilities for x\n",
    "    cem_pred_fn = lambda x: np.array(explained_model.predict_proba(x)[0])#explained_model.predict_proba(x)[0][0][::-1].reshape(1, 2)#np.array([explained_model.predict_proba(x)[0][0][1], explained_model.predict_proba(x)[0][0][0]])\n",
    "\n",
    "    cem = CEM(cem_pred_fn, mode, shape, kappa=0.0, beta=0.1, feature_range=feature_ranges, \n",
    "            update_num_grad=update_num_grad, clip=clip, no_info_val=-0.0, c_init=c_init,\n",
    "            c_steps=10, learning_rate_init=.1, max_iterations=10, eps=eps_cem\n",
    "            )\n",
    "else:\n",
    "    mode = 'PN'  # 'PN' (pertinent negative) or 'PP' (pertinent positive)\n",
    "    kappa = .3 # minimum difference needed between the prediction probability for the perturbed instance on the\n",
    "                # class predicted by the original instance and the max probability on the other classes\n",
    "                # in order for the first loss term to be minimized\n",
    "    beta = .1  # weight of the L1 loss term\n",
    "    c_init = 10  # initial weight c of the loss term encouraging to predict a different class (PN) or\n",
    "                # the same class (PP) for the perturbed instance compared to the original instance to be explained\n",
    "   \n",
    "    c_steps = 10  # nb of updates for c\n",
    "    max_iterations = 1000  # nb of iterations per value of c\n",
    "    # feature_range = (train_dataset_sparse_normalized.to_numpy().min(axis=0).reshape(shape),  # feature range for the perturbed instance\n",
    "    #                 train_dataset_sparse_normalized.to_numpy().max(axis=0).reshape(shape))  # can be either a float or array of shape (1xfeatures)\n",
    "    #feature_range = (train_dataset_sparse_normalized.to_numpy().min(),train_dataset_sparse_normalized.to_numpy().max())  # can be either a float or array of shape (1xfeatures)\n",
    "      # gradient clipping\n",
    "    lr_init = 1e-2  # initial learning rate\n",
    "\n",
    "    # initialize CEM explainer and explain instance\n",
    "    cem = CEM(explained_model, mode, shape, kappa=kappa, beta=beta, feature_range=feature_ranges,\n",
    "            max_iterations=max_iterations, c_init=c_init, c_steps=c_steps,\n",
    "            learning_rate_init=lr_init, clip=clip, no_info_val=0.0\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cem.fit(train_dataset_sparse_normalized.to_numpy(), no_info_type='median')  # we need to define what feature values contain the least\n",
    "                                                                    # info wrt predictions\n",
    "                                                                    # here we will naively assume that the feature-wise median\n",
    "                                                                    # contains no info; domain knowledge helps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cem_explanation = cem.explain(query_instance_sparse_normalized.to_numpy(), verbose=True)\n",
    "\n",
    "cem_cf_df = pd.DataFrame(cem_explanation.PN, columns=constr['features_order_after_split'])\n",
    "\n",
    "# Inverse min-max normalization\n",
    "cem_cf_df = inverse_min_max_normalization(\n",
    "    _df=cem_cf_df,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "# Inverse transform to sparse\n",
    "cem_cf_df = inverse_transform_to_sparse(\n",
    "    sparse_df=cem_cf_df,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    categorical_features=constr['categorical_features_nonsplit'],\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "cem_cf_df['explainer'] = 'Cem'\n",
    "print(all_counterfactuals.shape)\n",
    "all_counterfactuals = pd.concat([all_counterfactuals, cem_cf_df], ignore_index=True)\n",
    "print(all_counterfactuals.shape)\n",
    "\n",
    "cem_cf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFPROTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from alibi.explainers import CounterfactualProto\n",
    "\n",
    "# import pickle\n",
    "# with open('../models/adult_RF.pkl', 'rb') as f:\n",
    "#     cfprot_model = pickle.load(f)\n",
    "# predict_fnct = lambda x: cfprot_model.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_vars_ord = {}\n",
    "# for i, cat in enumerate(constr['categorical_features_nonsplit']):\n",
    "#     start_index = np.argwhere(cat == train_dataset.columns.to_numpy())[0][0]\n",
    "#     unique = len(np.unique(train_dataset[cat]))\n",
    "#     cat_vars_ord[start_index] = unique\n",
    "# print(cat_vars_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_vars_ohe = {}\n",
    "# for f in constr['categorical_features_nonsplit']:\n",
    "#     indx = constr['feature_first_occurrence_after_split'][f]\n",
    "#     cnt = constr['features_count_nonsplit'][f] \n",
    "#     cat_vars_ohe[indx] = cnt\n",
    "# cat_vars_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfProto = CounterfactualProto(predict_fnct,\n",
    "#                          query_instance_sparse_normalized.shape,\n",
    "#                          cat_vars=cat_vars_ohe,\n",
    "#                          ohe=True,  # OHE flag\n",
    "#                          max_iterations=500,\n",
    "#                          beta=0.01,\n",
    "#                          feature_range=(0.0, 1.0),\n",
    "#                         #  use_kdtree=True,\n",
    "#                          theta= 10.,\n",
    "#                          c_init=1.0,\n",
    "#                          c_steps=5,\n",
    "#                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cfProto.fit(train_dataset_sparse_normalized.to_numpy().astype('float64'), d_type='abdm', trustscore_kwargs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanation = cfProto.explain(query_instance_sparse_normalized.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization_helpers import get_scores\n",
    "from visualization_helpers import remove_duplicates\n",
    "\n",
    "all_counterfactuals = remove_duplicates(all_counterfactuals)\n",
    "print('Counterfactuals: ', all_counterfactuals.shape)\n",
    "\n",
    "# Transform counterfactuals to sparse\n",
    "counterfactuals_sparse = transform_to_sparse(\n",
    "    _df=all_counterfactuals,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    categorical_features=constr['categorical_features_nonsplit'],\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "# Normalize counterfactuals\n",
    "counterfactuals_sparse_normalized = min_max_normalization(\n",
    "    _df=counterfactuals_sparse,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "\n",
    "# Transform query instance to sparse\n",
    "query_instance_sparse = transform_to_sparse(\n",
    "    _df=query_instance,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    categorical_features=constr['categorical_features_nonsplit'],\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "# Normalize query instance sparse\n",
    "query_instance_sparse_normalized = min_max_normalization(\n",
    "    _df=query_instance_sparse,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "# Mask non actionable features\n",
    "mask_indices = [1 if any([act in x for act in constr['actionable_features']]) else 0 for x in constr['features_order_after_split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counterfactuals.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactuals_sparse_normalized.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explained_model_backend == 'sklearn':\n",
    "\n",
    "    cems = all_counterfactuals[all_counterfactuals['explainer'] == 'Cem'].index.tolist()\n",
    "    wachters = all_counterfactuals[all_counterfactuals['explainer'] == 'wachter'].index.tolist()\n",
    "    cadexes = all_counterfactuals[all_counterfactuals['explainer'] == 'Cadex'].index.tolist()\n",
    "    fimaps = all_counterfactuals[all_counterfactuals['explainer'] == 'Fimap'].index.tolist()\n",
    "\n",
    "\n",
    "    print('Orginal x: ',explained_model.predict_proba(query_instance_sparse_normalized)[0] )\n",
    "\n",
    "    if len(cems) > 0:\n",
    "        print('cem: ', explained_model.predict_proba(counterfactuals_sparse_normalized.iloc[cems].to_numpy().reshape(-1, 85))[0])\n",
    "    if len(wachters) > 0:\n",
    "        print('wachters: ', explained_model.predict_proba(counterfactuals_sparse_normalized.iloc[wachters].to_numpy().reshape(-1, 85))[0])\n",
    "    if len(cadexes) > 0:\n",
    "        print('cadexes: ', explained_model.predict_proba(counterfactuals_sparse_normalized.iloc[cadexes].to_numpy().reshape(-1, 85))[0])\n",
    "    if len(fimaps) > 0:\n",
    "        print('fimaps: ', explained_model.predict_proba(counterfactuals_sparse_normalized.iloc[fimaps].to_numpy().reshape(-1, 85))[0])\n",
    "\n",
    "if explained_model_backend == 'tensorflow':\n",
    "\n",
    "    cems = all_counterfactuals[all_counterfactuals['explainer'] == 'Cem'].index.tolist()\n",
    "    wachters = all_counterfactuals[all_counterfactuals['explainer'] == 'wachter'].index.tolist()\n",
    "    cadexes = all_counterfactuals[all_counterfactuals['explainer'] == 'Cadex'].index.tolist()\n",
    "    fimaps = all_counterfactuals[all_counterfactuals['explainer'] == 'Fimap'].index.tolist()\n",
    "\n",
    "    print(cems)\n",
    "\n",
    "\n",
    "    print('Orginal x: ',explained_model.predict(query_instance_sparse_normalized) )\n",
    "\n",
    "    if len(cems) > 0:\n",
    "        print(f'cem: {np.round(explained_model.predict(counterfactuals_sparse_normalized.iloc[cems].to_numpy().reshape(-1, 85)), 3)}')\n",
    "    if len(wachters) > 0:\n",
    "        print(f'wachters: {np.round(explained_model.predict(counterfactuals_sparse_normalized.iloc[wachters].to_numpy().reshape(-1, 85)), 3)}')\n",
    "    if len(cadexes) > 0:\n",
    "        print(f'cadexes: {np.round(explained_model.predict(counterfactuals_sparse_normalized.iloc[cadexes].to_numpy().reshape(-1, 85)), 3)}')\n",
    "    if len(fimaps) > 0:\n",
    "        print(f'fimaps: {np.round(explained_model.predict(counterfactuals_sparse_normalized.iloc[fimaps].to_numpy().reshape(-1, 85)), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization_helpers import filter_non_valid\n",
    "from visualization_helpers import filter_non_actionable_features\n",
    "\n",
    "# # SKLEARN\n",
    "# with open('../models/adult_RF.pkl', 'rb') as f:\n",
    "#     model = pickle.load(f)\n",
    "\n",
    "# # TENSORFLOW\n",
    "# model = tf.keras.models.load_model('../models/adult_NN/')\n",
    "\n",
    "if explained_model_backend == 'sklearn':\n",
    "    # SKLEARN\n",
    "    with open('models/adult_RF.pkl', 'rb') as f:\n",
    "        explained_model = pickle.load(f)\n",
    "else: \n",
    "    # TENSORFLOW\n",
    "    explained_model = tf.keras.models.load_model('models/adult_NN/')\n",
    "\n",
    "predict_fn = lambda x: explained_model.predict(x)\n",
    "\n",
    "valid_counterfactuals_sparse_normalized = filter_non_valid(predict_fn, query_instance_sparse_normalized, counterfactuals_sparse_normalized)\n",
    "valid_counterfactuals_sparse_normalized.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter not feasible (data is min-max normalized, so values shouldn't be less than zero)\n",
    "# not_feasible = np.where(np.sum(counterfactuals_sparse_normalized < 0, axis=1) > 0)[0]\n",
    "not_feasible = []\n",
    "\n",
    "indices_to_keep = list(filter(lambda x: x not in not_feasible, valid_counterfactuals_sparse_normalized.index.tolist()))\n",
    "valid_counterfactuals_sparse_normalized = counterfactuals_sparse_normalized.iloc[indices_to_keep]\n",
    "print(indices_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_counterfactuals_sparse_normalized.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counterfactuals.iloc[indices_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_counterfactuals = all_counterfactuals.iloc[indices_to_keep]\n",
    "valid_counterfactuals.reset_index(drop=True, inplace=True)\n",
    "valid_counterfactuals = filter_non_actionable_features(valid_counterfactuals, query_instance, constr['non_actionable_features'], constr['categorical_features_nonsplit'], constr['continuous_features_nonsplit'])\n",
    "valid_counterfactuals_sparse_normalized = valid_counterfactuals_sparse_normalized.iloc[valid_counterfactuals.index.tolist()]\n",
    "valid_counterfactuals_sparse_normalized.reset_index(drop=True, inplace=True)\n",
    "valid_counterfactuals.reset_index(drop=True, inplace=True)\n",
    "valid_counterfactuals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add income column\n",
    "valid_counterfactuals['income'] = np.argmax(predict_fn(valid_counterfactuals_sparse_normalized.to_numpy()[0:1]))\n",
    "valid_counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores_df = get_scores(valid_counterfactuals_sparse_normalized.to_numpy(), query_instance_sparse_normalized, train_dataset_sparse_normalized, train_dataset['income'], mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_predicted_classes = np.argmax(predict_fn(train_dataset_sparse_normalized), axis=1)\n",
    "x_predicted_class = np.argmax(predict_fn(query_instance_sparse_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train_dataset.drop(['income'], axis=1).columns.tolist()\n",
    "continous_indices = list()\n",
    "categorical_indices = list()\n",
    "\n",
    "for col in constr['continuous_features_nonsplit']:\n",
    "    continous_indices += [cols.index(col)]\n",
    "\n",
    "for col in constr['categorical_features_nonsplit']:\n",
    "    categorical_indices += [cols.index(col)]\n",
    "\n",
    "print(continous_indices)\n",
    "print(categorical_indices)\n",
    "print('Proper indices extracted: ', len(categorical_indices + continous_indices) == len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferences = [0, 4, 2, 3, 5, 1]\n",
    "\n",
    "print(f'Preferences: {np.array(cols)[preferences]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.scores import get_scores\n",
    "\n",
    "\n",
    "scores_df = get_scores(\n",
    "    cfs=valid_counterfactuals.drop(['income', 'explainer'], axis=1).to_numpy().astype('<U11'),\n",
    "    cf_predicted_classes=valid_counterfactuals['income'].to_numpy(),\n",
    "    x=query_instance.to_numpy()[0].astype('<U11'),\n",
    "    x_predicted_class=x_predicted_class,\n",
    "    training_data=train_dataset.drop(['income'], axis=1).to_numpy().astype('<U11'),\n",
    "    training_data_predicted_classes=train_data_predicted_classes,\n",
    "    continous_indices=continous_indices,\n",
    "    categorical_indices=categorical_indices,\n",
    "    preferences_ranking=preferences,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df['explainer'] = valid_counterfactuals['explainer']\n",
    "scores_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_to_plot = scores_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_to_plot.explainer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sklearn' in str(type(explained_model)):\n",
    "    explained_model_name = 'RF'\n",
    "else:\n",
    "    explained_model_name = 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimization_direction(metric_name: str) -> str:\n",
    "    cost_criteria = ['feasibility', 'proximity', 'features']\n",
    "    gain_criteria = ['discriminative', 'dcg']\n",
    "\n",
    "    cost = any([True if x.lower() in metric_name.lower() else False for x in cost_criteria])\n",
    "    if cost:\n",
    "        return 'min'\n",
    "    else:\n",
    "        return 'max'\n",
    "        \n",
    "get_optimization_direction('Feasibility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from visualization_helpers import get_pareto_frontier_mask\n",
    "from pareto import get_pareto_optimal_mask\n",
    "\n",
    "#metrics_to_plot = ['proximity', 'features_changed', 'feasibility', 'dispreference_dcg', 'non_discriminative_power']\n",
    "metrics_to_plot = scores_to_plot.drop(['explainer'], axis=1).columns.tolist()\n",
    "\n",
    "n = len(metrics_to_plot)\n",
    "\n",
    "fig, ax = plt.subplots(n, n, figsize=(3.5*n, 3*n))\n",
    "\n",
    "colors = ['r', 'g', 'b', 'y', 'c', 'm', 'k', 'w']\n",
    "markers = ['s', 'o', 'v', '+', '*', 'p', 'P', 'X', 'D', '>']\n",
    "labels = []\n",
    "\n",
    "ax = ax.flatten()\n",
    "\n",
    "for plot_round in ['nonpareto', 'pareto']:\n",
    "    for i, other_metric in enumerate(metrics_to_plot):\n",
    "        for j, metric in enumerate(metrics_to_plot):\n",
    "\n",
    "            all_x = scores_to_plot[metric].to_numpy()\n",
    "            all_y = scores_to_plot[other_metric].to_numpy()\n",
    "            to_check = np.array([all_x, all_y], dtype=np.float64).T\n",
    "\n",
    "            # Get pareto frontiers mask\n",
    "            metric_direction = get_optimization_direction(metric)\n",
    "            other_metric_direction = get_optimization_direction(other_metric)\n",
    "            optimization_directions = [metric_direction, other_metric_direction]\n",
    "            all_pareto = get_pareto_optimal_mask(data=to_check, optimization_direction=optimization_directions).astype('bool')\n",
    "\n",
    "\n",
    "\n",
    "            ax[i*n+j].grid()\n",
    "\n",
    "            for k, explainer in enumerate(scores_to_plot['explainer'].value_counts().sort_values(ascending=True).index.tolist()):\n",
    "\n",
    "                mask = scores_to_plot['explainer'] == explainer\n",
    "                pareto = all_pareto[mask]\n",
    "\n",
    "                x = scores_to_plot[mask][metric].to_numpy()\n",
    "                y = scores_to_plot[mask][other_metric].to_numpy()\n",
    "            \n",
    "                if plot_round == 'nonpareto':\n",
    "                    if i == j:\n",
    "                        ax[i*n+j].hist(x, color=colors[k], label=explainer, alpha=0.5)\n",
    "                        ax[i*n+j].legend()\n",
    "                    else:\n",
    "                        ax[i*n+j].scatter(x[~pareto], y[~pareto], color='steelblue', marker=markers[k], label=explainer)\n",
    "                elif plot_round == 'pareto' and i!=j:\n",
    "                    ax[i*n+j].scatter(x[pareto], y[pareto], color='orange', marker=markers[k])\n",
    "\n",
    "                    if i < j:\n",
    "                        print(f'For explainer: {explainer} and metrics {metric}, {other_metric}, paretos: {sum(pareto)} out of {len(pareto)}')\n",
    "                        # print(f'{scores_df[scores_df[\"explainer\"] == explainer][[metric, other_metric]][pareto]}')\n",
    "            \n",
    "            ax[i*n+j].set_xlabel(f'({metric_direction}) {metric}')\n",
    "            ax[i*n+j].set_ylabel(f'({other_metric_direction}) {other_metric}')\n",
    "    # plt.title('Proximity vs Dispreference DCG \\n(Pareto front in orange). \\nLower is better.')\n",
    "\n",
    "handles, labels = ax[1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right')\n",
    "\n",
    "counts = scores_df['explainer'].value_counts()\n",
    "\n",
    "plt.suptitle(f'Pareto frontiers of the counterfactuals (lower is better)\\nExplained model: {explained_model_name}\\nDataset: {dataset_name}\\nCounterfactuals by method {counts.to_dict()}\\n')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'images/{dataset_name}/{explained_model_name}/{dataset_name}_{explained_model_name}_{instance_to_explain_index}_pairplot_with_frontiers.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_to_plot.to_csv(f'cf_scores_tmp-{explained_model_name}-{dataset_name}-{instance_to_explain_index}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_df[['feasibility', 'features_changed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = 'feasibility'\n",
    "# other_metric = 'features_changed'\n",
    "# all_x = scores_df[metric].to_numpy()\n",
    "# all_y = scores_df[other_metric].to_numpy()\n",
    "# to_check = np.array([all_x, all_y], dtype=np.float64).T\n",
    "# all_pareto = get_pareto_frontier_mask(to_check)\n",
    "# scores_df[[metric, other_metric, 'explainer']][all_pareto]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_df['explainer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "\n",
    "# metric = 'Proximity'\n",
    "# other_metric = 'Features Changed (normalized)'\n",
    "# other_other_metric = 'Feasibility'\n",
    "\n",
    "# all_x = scores_to_plot[metric].to_numpy()\n",
    "# all_y = scores_to_plot[other_metric].to_numpy()\n",
    "# all_z = scores_to_plot[other_other_metric].to_numpy()\n",
    "# to_check = np.array([all_x, all_y, all_z], dtype=np.float64).T\n",
    "\n",
    "# # Get pareto frontiers mask\n",
    "# metric_direction = get_optimization_direction(metric)\n",
    "# other_metric_direction = get_optimization_direction(other_metric)\n",
    "# other_other_metric_direction = get_optimization_direction(other_other_metric)\n",
    "# optimization_directions = [metric_direction, other_metric_direction, other_other_metric_direction]\n",
    "# all_pareto = get_pareto_optimal_mask(data=to_check, optimization_direction=optimization_directions).astype('bool')\n",
    "\n",
    "# for plot_round in ['nonpareto', 'pareto']:\n",
    "#     for k, explainer in enumerate(scores_to_plot['explainer'].value_counts().sort_values(ascending=True).index.tolist()):\n",
    "\n",
    "#         mask = scores_to_plot['explainer'] == explainer\n",
    "#         pareto = all_pareto[mask]\n",
    "\n",
    "#         x = scores_to_plot[mask][metric].to_numpy()\n",
    "#         y = scores_to_plot[mask][other_metric].to_numpy()\n",
    "#         z = scores_to_plot[mask][other_other_metric].to_numpy()\n",
    "\n",
    "#         if plot_round == 'nonpareto':\n",
    "#             ax.scatter(x[~pareto], y[~pareto], z[~pareto], color='steelblue', marker=markers[k], label=explainer)\n",
    "#         elif plot_round == 'pareto':\n",
    "#             ax.scatter(x[pareto], y[pareto], z[pareto], color='orange', marker=markers[k])\n",
    "         \n",
    "# ax.set_xlabel(f'({metric_direction}) {metric}')\n",
    "# ax.set_ylabel(f'({other_metric_direction}) {other_metric}')\n",
    "# ax.set_zlabel(f'({other_other_metric_direction}) {other_other_metric}')\n",
    "\n",
    "# plt.title(f'Pareto frontiers of the counterfactuals (lower is better)\\nExplained model: {explained_model_name}\\nDataset: {dataset_name}\\nCounterfactuals by method {counts.to_dict()}\\n')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 | packaged by conda-forge | (default, Nov 10 2021, 13:20:59) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "81d0d6994cbc9a338f9a3d74dedf5823f97cddc544b8ee56d3ad85196f930114"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

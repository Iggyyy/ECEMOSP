{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET THIS VARIABLE IF \n",
    "explained_model_backend = 'tensorflow' # 'sklearn' or 'tensorflow'\n",
    "\n",
    "# WARNING REMEMEBER TO CHANGE MANUALLY CFEC MODEL LOADING IF SOME CHANGES APPEAR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hours.per.week', 'age', 'capital.loss', 'education.num',\n",
       "       'capital.gain', 'workclass', 'marital.status', 'occupation', 'race',\n",
       "       'sex', 'native.country', 'income'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from utils.transformations import min_max_normalization, inverse_min_max_normalization, transform_to_sparse, inverse_transform_to_sparse\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning) #Ignore sklearn \"RF fitted with FeatureNames\"\n",
    "\n",
    "train_dataset = pd.read_csv(\"../data/adult.csv\")\n",
    "dataset_name = 'adult'\n",
    "instance_to_explain_index = 890\n",
    "\n",
    "with open('../data/adult_constraints.json', 'r') as f:\n",
    "    constr = json.load(f)\n",
    "\n",
    "if explained_model_backend == 'sklearn':\n",
    "    # SKLEARN\n",
    "    with open('../models/adult_RF.pkl', 'rb') as f:\n",
    "        explained_model = pickle.load(f)\n",
    "else: \n",
    "    # TENSORFLOW\n",
    "    explained_model = tf.keras.models.load_model('../models/adult_NN/')\n",
    "\n",
    "\n",
    "train_dataset = train_dataset[constr['features_order_nonsplit']]\n",
    "train_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionable_mask_indices_sparse = [1 if any([act in x for act in constr['actionable_features']]) else 0 for x in constr['features_order_after_split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_instance = train_dataset.drop(columns=\"income\")[instance_to_explain_index:instance_to_explain_index+1]\n",
    "\n",
    "all_counterfactuals = pd.DataFrame(columns=train_dataset.columns.tolist() + ['explainer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>age</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>education.num</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>workclass</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>native.country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>1887</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     hours.per.week  age  capital.loss  education.num  capital.gain  \\\n",
       "890              40   35          1887             14             0   \n",
       "\n",
       "     workclass      marital.status      occupation   race   sex native.country  \n",
       "890  Local-gov  Married-civ-spouse  Prof-specialty  White  Male  United-States  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dataset to sparse\n",
    "train_dataset_sparse = transform_to_sparse(\n",
    "    _df=train_dataset.drop(columns=\"income\"),\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    categorical_features=constr['categorical_features_nonsplit'],\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "# Min-max normalization\n",
    "train_dataset_sparse_normalized = min_max_normalization(\n",
    "    _df=train_dataset_sparse,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "query_instance_sparse_normalized = train_dataset_sparse_normalized[instance_to_explain_index:instance_to_explain_index+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 93ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.2027573e-04, 9.9977976e-01]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_model.predict(query_instance_sparse_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.87s/it]\n"
     ]
    }
   ],
   "source": [
    "from dice import DiceModel\n",
    "\n",
    "if explained_model_backend == 'sklearn':\n",
    "    dice_model = DiceModel(\n",
    "        train_dataset=train_dataset,\n",
    "        continuous_features=constr['continuous_features_nonsplit'],\n",
    "        categorical_features=constr['categorical_features_nonsplit'],\n",
    "        target=constr['target_feature'],\n",
    "        backend='sklearn',\n",
    "        model=explained_model\n",
    "    )\n",
    "else:\n",
    "    dice_model = DiceModel(\n",
    "        train_dataset=train_dataset,\n",
    "        continuous_features=constr['continuous_features_nonsplit'],\n",
    "        categorical_features=constr['categorical_features_nonsplit'],\n",
    "        target=constr['target_feature'],\n",
    "        backend='TF2',\n",
    "        model=explained_model\n",
    "    )\n",
    "\n",
    "dice_counterfactuals_df = dice_model.generate_counterfactuals(\n",
    "    query_instance=query_instance,\n",
    "    total_CFs=50,\n",
    "    desired_class='opposite',\n",
    "    features_to_vary=constr['actionable_features'],\n",
    "    permitted_range=constr['feature_ranges'],\n",
    ")\n",
    "\n",
    "dice_counterfactuals_df['explainer'] = 'dice'\n",
    "all_counterfactuals = pd.concat([all_counterfactuals, dice_counterfactuals_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counterfactuals.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(actionable_mask_indices_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n"
     ]
    }
   ],
   "source": [
    "print(np.where(actionable_mask_indices_sparse)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cfec_ece import CfecEceModel \n",
    "\n",
    "train_dataset_sparse_normalized_subsample = train_dataset_sparse_normalized.sample(frac=1.0)\n",
    "\n",
    "if explained_model_backend == 'sklearn':\n",
    "    cfec_model = CfecEceModel(\n",
    "        train_data_normalized=train_dataset_sparse_normalized_subsample,\n",
    "        constraints_dictionary=constr,\n",
    "        model_path='../models/adult_RF.pkl',\n",
    "        model_backend='sklearn',\n",
    "        fimap_load_s_g_full_id=f'adult_sklearn|2023-01-17',\n",
    "        #fimap_save_s_q_prefix='adult_sklearn',\n",
    "        columns_to_change=np.where(actionable_mask_indices_sparse)[0].tolist(),\n",
    "        )\n",
    "else:\n",
    "    cfec_model = CfecEceModel(\n",
    "        train_data_normalized=train_dataset_sparse_normalized_subsample,\n",
    "        constraints_dictionary=constr,\n",
    "        model_path='../models/adult_NN/',\n",
    "        model_backend='tensorflow',\n",
    "        fimap_load_s_g_full_id=f'adult_tensorflow|2023-01-17',\n",
    "        #fimap_save_s_q_prefix='adult_tensorflow',\n",
    "        columns_to_change=np.where(actionable_mask_indices_sparse)[0].tolist(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfec_counterfactuals_raw, list_cfs_explainers = cfec_model.generate_counterfactuals(query_instance=query_instance_sparse_normalized.iloc[0])\n",
    "cfec_counterfactuals_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not allow for negative values\n",
    "cfec_counterfactuals_raw[cfec_counterfactuals_raw < 0] = 0\n",
    "\n",
    "# Inverse min-max normalization\n",
    "cfec_counterfactuals = inverse_min_max_normalization(\n",
    "    _df=cfec_counterfactuals_raw,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "# Inverse transform to sparse\n",
    "cfec_counterfactuals = inverse_transform_to_sparse(\n",
    "    sparse_df=cfec_counterfactuals,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    categorical_features=constr['categorical_features_nonsplit'],\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cfs_explainers = list(map(lambda x: 'cadex' if 'cadex' in str.lower(x) else 'fimap', list_cfs_explainers))\n",
    "cfec_counterfactuals['explainer'] = list_cfs_explainers\n",
    "all_counterfactuals = pd.concat([all_counterfactuals, cfec_counterfactuals], ignore_index=True)\n",
    "cfec_counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_instance_sparse_normalized[cfec_counterfactuals_raw.columns[39:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfec_counterfactuals_raw[cfec_counterfactuals_raw.columns[39:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WACHTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ranges = (\n",
    "    train_dataset_sparse_normalized.to_numpy().min(axis=0),\n",
    "    train_dataset_sparse_normalized.to_numpy().max(axis=0),\n",
    ")\n",
    "non_actionable_indices = ~np.array(actionable_mask_indices_sparse, dtype='bool')\n",
    "feature_ranges[0][non_actionable_indices] = query_instance_sparse_normalized.to_numpy()[0][non_actionable_indices]\n",
    "feature_ranges[1][non_actionable_indices] = query_instance_sparse_normalized.to_numpy()[0][non_actionable_indices]\n",
    "feature_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_impl import AlibiWachter\n",
    "\n",
    "continous = len(constr['continuous_features_nonsplit'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if explained_model_backend == 'sklearn':\n",
    "    max_iter = 100\n",
    "    max_lam_steps=10\n",
    "    lam_init=0.001\n",
    "    lr_init=0.1\n",
    "    early_stop=50\n",
    "    tolerance=0.4\n",
    "    target_proba=1.0\n",
    "\n",
    "    wachter_model = AlibiWachter('../models/adult_RF.pkl', 'sklearn', \n",
    "    query_instance_sparse_normalized.shape, feature_ranges=feature_ranges,\n",
    "    max_iter=max_iter, max_lam_steps=max_lam_steps, lam_init=lam_init, \n",
    "    learning_rate_init=lr_init, early_stop=early_stop, tolerance=tolerance,\n",
    "    target_proba=target_proba,\n",
    "    )\n",
    "else:\n",
    "    #eps_wachter = np.array([[0.01] * continous + [0.01] * (len(train_dataset_sparse_normalized.columns) - continous)]) * (np.array(actionable_mask_indices_sparse, dtype=int) + 0.001)\n",
    "    wachter_model = AlibiWachter('../models/adult_NN/', 'tensorflow', query_instance_sparse_normalized.shape, target_proba=1.0, feature_ranges=feature_ranges)\n",
    "    \n",
    "explanation = wachter_model.generate_counterfactuals(query_instance_sparse_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.cf['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.cf['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wachter_counterfactuals = [explanation.cf['X']]\n",
    "for key, lst in explanation['data']['all'].items():\n",
    "    if lst:\n",
    "        for cf in lst:\n",
    "            wachter_counterfactuals.append(cf['X'])\n",
    "\n",
    "wachter_counterfactuals = np.array(wachter_counterfactuals).reshape(-1, query_instance_sparse_normalized.shape[1])\n",
    "\n",
    "wachter_counterfactuals_df = pd.DataFrame(wachter_counterfactuals, columns=constr['features_order_after_split'])\n",
    "\n",
    "# Inverse min-max normalization\n",
    "wachter_counterfactuals_df = inverse_min_max_normalization(\n",
    "    _df=wachter_counterfactuals_df,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "# Inverse transform to sparse\n",
    "wachter_counterfactuals_df = inverse_transform_to_sparse(\n",
    "    sparse_df=wachter_counterfactuals_df,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    categorical_features=constr['categorical_features_nonsplit'],\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "wachter_counterfactuals_df['explainer'] = 'wachter'\n",
    "\n",
    "# Reduce number of Wachter counterfactuals because they are almost the same\n",
    "sampled_wachter_cfs = wachter_counterfactuals_df.sample(min(len(wachter_counterfactuals_df), 10))\n",
    "\n",
    "#sampled_wachter_cfs = sampled_wachter_cfs.append(wachter_counterfactuals_df.iloc[0])\n",
    "\n",
    "all_counterfactuals = pd.concat([all_counterfactuals, wachter_counterfactuals_df.iloc[0:1], sampled_wachter_cfs], ignore_index=True)\n",
    "\n",
    "wachter_counterfactuals_df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wachter_counterfactuals_df.iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wachter_counterfactuals_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi.explainers import CEM\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "if explained_model_backend == 'sklearn':\n",
    "    # SKLEARN\n",
    "    with open('../models/adult_RF.pkl', 'rb') as f:\n",
    "        explained_model = pickle.load(f)\n",
    "else: \n",
    "    # TENSORFLOW\n",
    "    explained_model = tf.keras.models.load_model('../models/adult_NN/')\n",
    "\n",
    "shape = query_instance_sparse_normalized.shape  # instance shape\n",
    "continous = len(constr['continuous_features_nonsplit'])\n",
    "clip = (-1000.,1000.)\n",
    "eps_cem = (\n",
    "        0.05,\n",
    "        np.array([[0.05] * continous + [1.0] * (len(train_dataset_sparse_normalized.columns) - continous)]) #* (np.array(actionable_mask_indices_sparse) + 0.001) # Dont allow changes on non-actionable features\n",
    "        )\n",
    "# eps_cem = (0.1, 0.1)\n",
    "\n",
    "if explained_model_backend == 'sklearn':\n",
    "    mode = 'PN'\n",
    "    #feature_range = (train_dataset_sparse_normalized.to_numpy().min(),  # feature range for the perturbed instance\n",
    "    #                   train_dataset_sparse_normalized.to_numpy().max()) \n",
    "    update_num_grad = 2\n",
    "    c_init = 15.  # initial weight c of the loss term encouraging to predict a different class (PN) or\n",
    "                # the same class (PP) for the perturbed instance compared to the original instance to be explained\n",
    "    # Return probabilities for x\n",
    "    cem_pred_fn = lambda x: np.array(explained_model.predict_proba(x)[0])#explained_model.predict_proba(x)[0][0][::-1].reshape(1, 2)#np.array([explained_model.predict_proba(x)[0][0][1], explained_model.predict_proba(x)[0][0][0]])\n",
    "\n",
    "    cem = CEM(cem_pred_fn, mode, shape, kappa=0.0, beta=0.1, feature_range=feature_ranges, \n",
    "            update_num_grad=update_num_grad, clip=clip, no_info_val=-0.0, c_init=c_init,\n",
    "            c_steps=10, learning_rate_init=.1, max_iterations=10, eps=eps_cem\n",
    "            )\n",
    "else:\n",
    "    mode = 'PN'  # 'PN' (pertinent negative) or 'PP' (pertinent positive)\n",
    "    kappa = .3 # minimum difference needed between the prediction probability for the perturbed instance on the\n",
    "                # class predicted by the original instance and the max probability on the other classes\n",
    "                # in order for the first loss term to be minimized\n",
    "    beta = .1  # weight of the L1 loss term\n",
    "    c_init = 10  # initial weight c of the loss term encouraging to predict a different class (PN) or\n",
    "                # the same class (PP) for the perturbed instance compared to the original instance to be explained\n",
    "   \n",
    "    c_steps = 10  # nb of updates for c\n",
    "    max_iterations = 1000  # nb of iterations per value of c\n",
    "    # feature_range = (train_dataset_sparse_normalized.to_numpy().min(axis=0).reshape(shape),  # feature range for the perturbed instance\n",
    "    #                 train_dataset_sparse_normalized.to_numpy().max(axis=0).reshape(shape))  # can be either a float or array of shape (1xfeatures)\n",
    "    #feature_range = (train_dataset_sparse_normalized.to_numpy().min(),train_dataset_sparse_normalized.to_numpy().max())  # can be either a float or array of shape (1xfeatures)\n",
    "      # gradient clipping\n",
    "    lr_init = 1e-2  # initial learning rate\n",
    "\n",
    "    # initialize CEM explainer and explain instance\n",
    "    cem = CEM(explained_model, mode, shape, kappa=kappa, beta=beta, feature_range=feature_ranges,\n",
    "            max_iterations=max_iterations, c_init=c_init, c_steps=c_steps,\n",
    "            learning_rate_init=lr_init, clip=clip, no_info_val=0.0\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cem.fit(train_dataset_sparse_normalized.to_numpy(), no_info_type='median')  # we need to define what feature values contain the least\n",
    "                                                                    # info wrt predictions\n",
    "                                                                    # here we will naively assume that the feature-wise median\n",
    "                                                                    # contains no info; domain knowledge helps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cem_explanation = cem.explain(query_instance_sparse_normalized.to_numpy(), verbose=True)\n",
    "\n",
    "cem_cf_df = pd.DataFrame(cem_explanation.PN, columns=constr['features_order_after_split'])\n",
    "\n",
    "# Inverse min-max normalization\n",
    "cem_cf_df = inverse_min_max_normalization(\n",
    "    _df=cem_cf_df,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "# Inverse transform to sparse\n",
    "cem_cf_df = inverse_transform_to_sparse(\n",
    "    sparse_df=cem_cf_df,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    categorical_features=constr['categorical_features_nonsplit'],\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "cem_cf_df['explainer'] = 'Cem'\n",
    "print(all_counterfactuals.shape)\n",
    "all_counterfactuals = pd.concat([all_counterfactuals, cem_cf_df], ignore_index=True)\n",
    "print(all_counterfactuals.shape)\n",
    "\n",
    "cem_cf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFPROTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from alibi.explainers import CounterfactualProto\n",
    "\n",
    "# import pickle\n",
    "# with open('../models/adult_RF.pkl', 'rb') as f:\n",
    "#     cfprot_model = pickle.load(f)\n",
    "# predict_fnct = lambda x: cfprot_model.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_vars_ord = {}\n",
    "# for i, cat in enumerate(constr['categorical_features_nonsplit']):\n",
    "#     start_index = np.argwhere(cat == train_dataset.columns.to_numpy())[0][0]\n",
    "#     unique = len(np.unique(train_dataset[cat]))\n",
    "#     cat_vars_ord[start_index] = unique\n",
    "# print(cat_vars_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_vars_ohe = {}\n",
    "# for f in constr['categorical_features_nonsplit']:\n",
    "#     indx = constr['feature_first_occurrence_after_split'][f]\n",
    "#     cnt = constr['features_count_nonsplit'][f] \n",
    "#     cat_vars_ohe[indx] = cnt\n",
    "# cat_vars_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfProto = CounterfactualProto(predict_fnct,\n",
    "#                          query_instance_sparse_normalized.shape,\n",
    "#                          cat_vars=cat_vars_ohe,\n",
    "#                          ohe=True,  # OHE flag\n",
    "#                          max_iterations=500,\n",
    "#                          beta=0.01,\n",
    "#                          feature_range=(0.0, 1.0),\n",
    "#                         #  use_kdtree=True,\n",
    "#                          theta= 10.,\n",
    "#                          c_init=1.0,\n",
    "#                          c_steps=5,\n",
    "#                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cfProto.fit(train_dataset_sparse_normalized.to_numpy().astype('float64'), d_type='abdm', trustscore_kwargs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanation = cfProto.explain(query_instance_sparse_normalized.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization_helpers import get_scores\n",
    "from visualization_helpers import remove_duplicates\n",
    "\n",
    "all_counterfactuals = remove_duplicates(all_counterfactuals)\n",
    "print('Counterfactuals: ', all_counterfactuals.shape)\n",
    "\n",
    "# Transform counterfactuals to sparse\n",
    "counterfactuals_sparse = transform_to_sparse(\n",
    "    _df=all_counterfactuals,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    categorical_features=constr['categorical_features_nonsplit'],\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "# Normalize counterfactuals\n",
    "counterfactuals_sparse_normalized = min_max_normalization(\n",
    "    _df=counterfactuals_sparse,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "\n",
    "# Transform query instance to sparse\n",
    "query_instance_sparse = transform_to_sparse(\n",
    "    _df=query_instance,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    categorical_features=constr['categorical_features_nonsplit'],\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "# Normalize query instance sparse\n",
    "query_instance_sparse_normalized = min_max_normalization(\n",
    "    _df=query_instance_sparse,\n",
    "    original_df=train_dataset.drop(columns=\"income\"),\n",
    "    continuous_features=constr['continuous_features_nonsplit']\n",
    ")\n",
    "\n",
    "# Mask non actionable features\n",
    "mask_indices = [1 if any([act in x for act in constr['actionable_features']]) else 0 for x in constr['features_order_after_split']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counterfactuals.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactuals_sparse_normalized.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if explained_model_backend == 'sklearn':\n",
    "\n",
    "    cems = all_counterfactuals[all_counterfactuals['explainer'] == 'Cem'].index.tolist()\n",
    "    wachters = all_counterfactuals[all_counterfactuals['explainer'] == 'wachter'].index.tolist()\n",
    "    cadexes = all_counterfactuals[all_counterfactuals['explainer'] == 'Cadex'].index.tolist()\n",
    "    fimaps = all_counterfactuals[all_counterfactuals['explainer'] == 'Fimap'].index.tolist()\n",
    "\n",
    "\n",
    "    print('Orginal x: ',explained_model.predict_proba(query_instance_sparse_normalized)[0] )\n",
    "\n",
    "    if len(cems) > 0:\n",
    "        print('cem: ', explained_model.predict_proba(counterfactuals_sparse_normalized.iloc[cems].to_numpy().reshape(-1, 85))[0])\n",
    "    if len(wachters) > 0:\n",
    "        print('wachters: ', explained_model.predict_proba(counterfactuals_sparse_normalized.iloc[wachters].to_numpy().reshape(-1, 85))[0])\n",
    "    if len(cadexes) > 0:\n",
    "        print('cadexes: ', explained_model.predict_proba(counterfactuals_sparse_normalized.iloc[cadexes].to_numpy().reshape(-1, 85))[0])\n",
    "    if len(fimaps) > 0:\n",
    "        print('fimaps: ', explained_model.predict_proba(counterfactuals_sparse_normalized.iloc[fimaps].to_numpy().reshape(-1, 85))[0])\n",
    "\n",
    "if explained_model_backend == 'tensorflow':\n",
    "\n",
    "    cems = all_counterfactuals[all_counterfactuals['explainer'] == 'Cem'].index.tolist()\n",
    "    wachters = all_counterfactuals[all_counterfactuals['explainer'] == 'wachter'].index.tolist()\n",
    "    cadexes = all_counterfactuals[all_counterfactuals['explainer'] == 'Cadex'].index.tolist()\n",
    "    fimaps = all_counterfactuals[all_counterfactuals['explainer'] == 'Fimap'].index.tolist()\n",
    "\n",
    "    print(cems)\n",
    "\n",
    "\n",
    "    print('Orginal x: ',explained_model.predict(query_instance_sparse_normalized) )\n",
    "\n",
    "    if len(cems) > 0:\n",
    "        print(f'cem: {np.round(explained_model.predict(counterfactuals_sparse_normalized.iloc[cems].to_numpy().reshape(-1, 85)), 3)}')\n",
    "    if len(wachters) > 0:\n",
    "        print(f'wachters: {np.round(explained_model.predict(counterfactuals_sparse_normalized.iloc[wachters].to_numpy().reshape(-1, 85)), 3)}')\n",
    "    if len(cadexes) > 0:\n",
    "        print(f'cadexes: {np.round(explained_model.predict(counterfactuals_sparse_normalized.iloc[cadexes].to_numpy().reshape(-1, 85)), 3)}')\n",
    "    if len(fimaps) > 0:\n",
    "        print(f'fimaps: {np.round(explained_model.predict(counterfactuals_sparse_normalized.iloc[fimaps].to_numpy().reshape(-1, 85)), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization_helpers import filter_non_valid\n",
    "from visualization_helpers import filter_non_actionable_features\n",
    "\n",
    "# # SKLEARN\n",
    "# with open('../models/adult_RF.pkl', 'rb') as f:\n",
    "#     model = pickle.load(f)\n",
    "\n",
    "# # TENSORFLOW\n",
    "# model = tf.keras.models.load_model('../models/adult_NN/')\n",
    "\n",
    "if explained_model_backend == 'sklearn':\n",
    "    # SKLEARN\n",
    "    with open('../models/adult_RF.pkl', 'rb') as f:\n",
    "        explained_model = pickle.load(f)\n",
    "else: \n",
    "    # TENSORFLOW\n",
    "    explained_model = tf.keras.models.load_model('../models/adult_NN/')\n",
    "\n",
    "predict_fn = lambda x: explained_model.predict(x)\n",
    "\n",
    "valid_counterfactuals_sparse_normalized = filter_non_valid(predict_fn, query_instance_sparse_normalized, counterfactuals_sparse_normalized)\n",
    "valid_counterfactuals_sparse_normalized.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter not feasible (data is min-max normalized, so values shouldn't be less than zero)\n",
    "# not_feasible = np.where(np.sum(counterfactuals_sparse_normalized < 0, axis=1) > 0)[0]\n",
    "not_feasible = []\n",
    "\n",
    "indices_to_keep = list(filter(lambda x: x not in not_feasible, valid_counterfactuals_sparse_normalized.index.tolist()))\n",
    "valid_counterfactuals_sparse_normalized = counterfactuals_sparse_normalized.iloc[indices_to_keep]\n",
    "print(indices_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_counterfactuals_sparse_normalized.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_counterfactuals.iloc[indices_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_counterfactuals = all_counterfactuals.iloc[indices_to_keep]\n",
    "valid_counterfactuals.reset_index(drop=True, inplace=True)\n",
    "valid_counterfactuals = filter_non_actionable_features(valid_counterfactuals, query_instance, constr['non_actionable_features'], constr['categorical_features_nonsplit'], constr['continuous_features_nonsplit'])\n",
    "valid_counterfactuals_sparse_normalized = valid_counterfactuals_sparse_normalized.iloc[valid_counterfactuals.index.tolist()]\n",
    "valid_counterfactuals_sparse_normalized.reset_index(drop=True, inplace=True)\n",
    "valid_counterfactuals.reset_index(drop=True, inplace=True)\n",
    "valid_counterfactuals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add income column\n",
    "valid_counterfactuals['income'] = np.argmax(predict_fn(valid_counterfactuals_sparse_normalized.to_numpy()[0:1]))\n",
    "valid_counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores_df = get_scores(valid_counterfactuals_sparse_normalized.to_numpy(), query_instance_sparse_normalized, train_dataset_sparse_normalized, train_dataset['income'], mask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_predicted_classes = np.argmax(predict_fn(train_dataset_sparse_normalized), axis=1)\n",
    "x_predicted_class = np.argmax(predict_fn(query_instance_sparse_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train_dataset.drop(['income'], axis=1).columns.tolist()\n",
    "continous_indices = list()\n",
    "categorical_indices = list()\n",
    "\n",
    "for col in constr['continuous_features_nonsplit']:\n",
    "    continous_indices += [cols.index(col)]\n",
    "\n",
    "for col in constr['categorical_features_nonsplit']:\n",
    "    categorical_indices += [cols.index(col)]\n",
    "\n",
    "print(continous_indices)\n",
    "print(categorical_indices)\n",
    "print('Proper indices extracted: ', len(categorical_indices + continous_indices) == len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferences = [0, 4, 2, 3, 5, 1]\n",
    "\n",
    "print(f'Preferences: {np.array(cols)[preferences]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.scores import get_scores\n",
    "\n",
    "\n",
    "scores_df = get_scores(\n",
    "    cfs=valid_counterfactuals.drop(['income', 'explainer'], axis=1).to_numpy().astype('<U11'),\n",
    "    cf_predicted_classes=valid_counterfactuals['income'].to_numpy(),\n",
    "    x=query_instance.to_numpy()[0].astype('<U11'),\n",
    "    x_predicted_class=x_predicted_class,\n",
    "    training_data=train_dataset.drop(['income'], axis=1).to_numpy().astype('<U11'),\n",
    "    training_data_predicted_classes=train_data_predicted_classes,\n",
    "    continous_indices=continous_indices,\n",
    "    categorical_indices=categorical_indices,\n",
    "    preferences_ranking=preferences,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df['explainer'] = valid_counterfactuals['explainer']\n",
    "scores_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_to_plot = scores_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_to_plot.explainer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sklearn' in str(type(explained_model)):\n",
    "    explained_model_name = 'RF'\n",
    "else:\n",
    "    explained_model_name = 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimization_direction(metric_name: str) -> str:\n",
    "    cost_criteria = ['feasibility', 'proximity', 'features']\n",
    "    gain_criteria = ['discriminative', 'dcg']\n",
    "\n",
    "    cost = any([True if x.lower() in metric_name.lower() else False for x in cost_criteria])\n",
    "    if cost:\n",
    "        return 'min'\n",
    "    else:\n",
    "        return 'max'\n",
    "        \n",
    "get_optimization_direction('Feasibility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from visualization_helpers import get_pareto_frontier_mask\n",
    "from pareto import get_pareto_optimal_mask\n",
    "\n",
    "#metrics_to_plot = ['proximity', 'features_changed', 'feasibility', 'dispreference_dcg', 'non_discriminative_power']\n",
    "metrics_to_plot = scores_to_plot.drop(['explainer'], axis=1).columns.tolist()\n",
    "\n",
    "n = len(metrics_to_plot)\n",
    "\n",
    "fig, ax = plt.subplots(n, n, figsize=(3.5*n, 3*n))\n",
    "\n",
    "colors = ['r', 'g', 'b', 'y', 'c', 'm', 'k', 'w']\n",
    "markers = ['s', 'o', 'v', '+', '*', 'p', 'P', 'X', 'D', '>']\n",
    "labels = []\n",
    "\n",
    "ax = ax.flatten()\n",
    "\n",
    "for plot_round in ['nonpareto', 'pareto']:\n",
    "    for i, other_metric in enumerate(metrics_to_plot):\n",
    "        for j, metric in enumerate(metrics_to_plot):\n",
    "\n",
    "            all_x = scores_to_plot[metric].to_numpy()\n",
    "            all_y = scores_to_plot[other_metric].to_numpy()\n",
    "            to_check = np.array([all_x, all_y], dtype=np.float64).T\n",
    "\n",
    "            # Get pareto frontiers mask\n",
    "            metric_direction = get_optimization_direction(metric)\n",
    "            other_metric_direction = get_optimization_direction(other_metric)\n",
    "            optimization_directions = [metric_direction, other_metric_direction]\n",
    "            all_pareto = get_pareto_optimal_mask(data=to_check, optimization_direction=optimization_directions).astype('bool')\n",
    "\n",
    "\n",
    "\n",
    "            ax[i*n+j].grid()\n",
    "\n",
    "            for k, explainer in enumerate(scores_to_plot['explainer'].value_counts().sort_values(ascending=True).index.tolist()):\n",
    "\n",
    "                mask = scores_to_plot['explainer'] == explainer\n",
    "                pareto = all_pareto[mask]\n",
    "\n",
    "                x = scores_to_plot[mask][metric].to_numpy()\n",
    "                y = scores_to_plot[mask][other_metric].to_numpy()\n",
    "            \n",
    "                if plot_round == 'nonpareto':\n",
    "                    if i == j:\n",
    "                        ax[i*n+j].hist(x, color=colors[k], label=explainer, alpha=0.5)\n",
    "                        ax[i*n+j].legend()\n",
    "                    else:\n",
    "                        ax[i*n+j].scatter(x[~pareto], y[~pareto], color='steelblue', marker=markers[k], label=explainer)\n",
    "                elif plot_round == 'pareto' and i!=j:\n",
    "                    ax[i*n+j].scatter(x[pareto], y[pareto], color='orange', marker=markers[k])\n",
    "\n",
    "                    if i < j:\n",
    "                        print(f'For explainer: {explainer} and metrics {metric}, {other_metric}, paretos: {sum(pareto)} out of {len(pareto)}')\n",
    "                        # print(f'{scores_df[scores_df[\"explainer\"] == explainer][[metric, other_metric]][pareto]}')\n",
    "            \n",
    "            ax[i*n+j].set_xlabel(f'({metric_direction}) {metric}')\n",
    "            ax[i*n+j].set_ylabel(f'({other_metric_direction}) {other_metric}')\n",
    "    # plt.title('Proximity vs Dispreference DCG \\n(Pareto front in orange). \\nLower is better.')\n",
    "\n",
    "handles, labels = ax[1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right')\n",
    "\n",
    "counts = scores_df['explainer'].value_counts()\n",
    "\n",
    "plt.suptitle(f'Pareto frontiers of the counterfactuals (lower is better)\\nExplained model: {explained_model_name}\\nDataset: {dataset_name}\\nCounterfactuals by method {counts.to_dict()}\\n')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../images/{dataset_name}/{explained_model_name}/{dataset_name}_{explained_model_name}_{instance_to_explain_index}_pairplot_with_frontiers.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_to_plot.to_csv(f'cf_scores_tmp-{explained_model_name}-{dataset_name}-{instance_to_explain_index}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_df[['feasibility', 'features_changed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = 'feasibility'\n",
    "# other_metric = 'features_changed'\n",
    "# all_x = scores_df[metric].to_numpy()\n",
    "# all_y = scores_df[other_metric].to_numpy()\n",
    "# to_check = np.array([all_x, all_y], dtype=np.float64).T\n",
    "# all_pareto = get_pareto_frontier_mask(to_check)\n",
    "# scores_df[[metric, other_metric, 'explainer']][all_pareto]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_df['explainer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "\n",
    "# metric = 'Proximity'\n",
    "# other_metric = 'Features Changed (normalized)'\n",
    "# other_other_metric = 'Feasibility'\n",
    "\n",
    "# all_x = scores_to_plot[metric].to_numpy()\n",
    "# all_y = scores_to_plot[other_metric].to_numpy()\n",
    "# all_z = scores_to_plot[other_other_metric].to_numpy()\n",
    "# to_check = np.array([all_x, all_y, all_z], dtype=np.float64).T\n",
    "\n",
    "# # Get pareto frontiers mask\n",
    "# metric_direction = get_optimization_direction(metric)\n",
    "# other_metric_direction = get_optimization_direction(other_metric)\n",
    "# other_other_metric_direction = get_optimization_direction(other_other_metric)\n",
    "# optimization_directions = [metric_direction, other_metric_direction, other_other_metric_direction]\n",
    "# all_pareto = get_pareto_optimal_mask(data=to_check, optimization_direction=optimization_directions).astype('bool')\n",
    "\n",
    "# for plot_round in ['nonpareto', 'pareto']:\n",
    "#     for k, explainer in enumerate(scores_to_plot['explainer'].value_counts().sort_values(ascending=True).index.tolist()):\n",
    "\n",
    "#         mask = scores_to_plot['explainer'] == explainer\n",
    "#         pareto = all_pareto[mask]\n",
    "\n",
    "#         x = scores_to_plot[mask][metric].to_numpy()\n",
    "#         y = scores_to_plot[mask][other_metric].to_numpy()\n",
    "#         z = scores_to_plot[mask][other_other_metric].to_numpy()\n",
    "\n",
    "#         if plot_round == 'nonpareto':\n",
    "#             ax.scatter(x[~pareto], y[~pareto], z[~pareto], color='steelblue', marker=markers[k], label=explainer)\n",
    "#         elif plot_round == 'pareto':\n",
    "#             ax.scatter(x[pareto], y[pareto], z[pareto], color='orange', marker=markers[k])\n",
    "         \n",
    "# ax.set_xlabel(f'({metric_direction}) {metric}')\n",
    "# ax.set_ylabel(f'({other_metric_direction}) {other_metric}')\n",
    "# ax.set_zlabel(f'({other_other_metric_direction}) {other_other_metric}')\n",
    "\n",
    "# plt.title(f'Pareto frontiers of the counterfactuals (lower is better)\\nExplained model: {explained_model_name}\\nDataset: {dataset_name}\\nCounterfactuals by method {counts.to_dict()}\\n')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "81d0d6994cbc9a338f9a3d74dedf5823f97cddc544b8ee56d3ad85196f930114"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
